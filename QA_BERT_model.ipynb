{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "QA BERT model.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "xz4W_-pMVJsX",
        "ovpD8XkfVpTU",
        "H2cYcF0_Vvxv",
        "CkHR8UBXVxyv"
      ],
      "authorship_tag": "ABX9TyNKTyq1aqwiQN0cLygLfvf1",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/supertime1/BERT/blob/main/QA_BERT_model.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xz4W_-pMVJsX"
      },
      "source": [
        "#1.Import Dependencies"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "orAMKxKkU_Vh"
      },
      "source": [
        "import os\r\n",
        "import tensorflow.keras as keras\r\n",
        "import tensorflow as tf\r\n",
        "import tensorflow_hub as hub\r\n",
        "import tensorflow_datasets as tfds\r\n",
        "import tensorflow_text as text  # A dependency of the preprocessing model\r\n",
        "import tensorflow_addons as tfa\r\n",
        "from official.nlp import optimization\r\n",
        "import numpy as np\r\n",
        "import json\r\n",
        "\r\n",
        "tf.get_logger().setLevel('ERROR')"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8xr0VeYgYyKp"
      },
      "source": [
        "import sys\r\n",
        "sys.path.append(r'C:\\Users\\57lzhang.US04WW4008\\Documents\\GitHub\\BERT')"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Zlt53u2UVlb-"
      },
      "source": [
        "#2.Preprocess the input data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_CSuglUN49nx"
      },
      "source": [
        "from tokenizers import BertWordPieceTokenizer\r\n",
        "import string\r\n",
        "import numpy as np"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kh_mG7LuzHso"
      },
      "source": [
        "class Sample:\r\n",
        "  def __init__(self, question, context, start_char_idx=None, answer_text=None, all_answers=None):\r\n",
        "    self.question = question\r\n",
        "    self.context = context\r\n",
        "    self.start_char_idx = start_char_idx\r\n",
        "    self.answer_text = answer_text\r\n",
        "    self.all_answers = all_answers\r\n",
        "    self.skip = False\r\n",
        "    self.start_token_idx = -1\r\n",
        "    self.end_token_idx = -1\r\n",
        "  \r\n",
        "  def preprocess(self):\r\n",
        "  # remove multipe whitespace between characters\r\n",
        "    context = \" \".join(str(self.context).split())\r\n",
        "    quesiton = \" \".join(str(self.question).split())\r\n",
        "    # tokenize context and question\r\n",
        "    tokenized_context = tokenizer.encode(context)\r\n",
        "    tokenized_question = tokenizer.encode(question)\r\n",
        "    # preprocess answer if this is a validation or training sample\r\n",
        "    if self.answer_text:\r\n",
        "      answer = \" \".join(str(self.answer_text).split())\r\n",
        "      # find the end character index\r\n",
        "      end_char_idx = self.start_char_idx + len(answer)\r\n",
        "      \r\n",
        "      # sometimes squad answers are off by a character or two – fix this\r\n",
        "      if context[self.start_char_idx-1:end_char_idx-1] == answer:\r\n",
        "          self.start_char_idx -= 1\r\n",
        "          end_char_idx -= 1\r\n",
        "      elif context[self.start_char_idx-2:end_char_idx-2] == answer:\r\n",
        "          self.start_char_idx -= 2\r\n",
        "          end_char_idx -= 2\r\n",
        "      \r\n",
        "      # check if end character index is in the context\r\n",
        "      if end_char_idx >= len(context):\r\n",
        "        self.skip = True\r\n",
        "        return\r\n",
        "      # mark all the character indexes in context that are also in answer\r\n",
        "      is_char_in_ans = [0] * len(context)\r\n",
        "      for idx in range(self.start_char_idx, end_char_idx):\r\n",
        "        is_char_in_ans[idx] = 1\r\n",
        "      # find all the tokens that are in the answers\r\n",
        "      ans_token_idx = []\r\n",
        "      for idx, (start, end) in enumerate(tokenized_context.offsets):\r\n",
        "        if sum(is_char_in_ans[start:end]) > 0:\r\n",
        "          ans_token_idx.append(idx)\r\n",
        "      if len(ans_token_idx) == 0:\r\n",
        "        self.skip = True\r\n",
        "        return\r\n",
        "      # get start and end token indexes\r\n",
        "      self.start_token_idx = ans_token_idx[0]\r\n",
        "      self.end_token_idx = ans_token_idx[-1]\r\n",
        "      # create inputs as usual\r\n",
        "      input_ids = tokenized_context.ids + tokenized_question.ids[1:]\r\n",
        "      attention_mask = [1] * len(input_ids)\r\n",
        "      token_type_ids = [0] * len(tokenized_context.ids) + [1] * len(tokenized_question.ids[1:])\r\n",
        "      padding_length = max_seq_length - len(input_ids)\r\n",
        "      # add padding if necessary\r\n",
        "      if padding_length > 0:\r\n",
        "        input_ids += [0] * padding_length\r\n",
        "        attention_mask += [0] * padding_length\r\n",
        "        token_type_ids += [0] * padding_length\r\n",
        "      else:\r\n",
        "        self.skip = True\r\n",
        "        return\r\n",
        "      \r\n",
        "      self.input_word_ids = input_ids\r\n",
        "      self.input_type_ids = token_type_ids\r\n",
        "      self.input_mask = attention_mask\r\n",
        "      self.context_token_to_char = tokenized_context.offsets"
      ],
      "execution_count": 44,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XP3Lxs7CDap7"
      },
      "source": [
        "def create_squad_examples(raw_data):\r\n",
        "    squad_examples = []\r\n",
        "    for item in raw_data[\"data\"]:\r\n",
        "      for para in item[\"paragraphs\"]:\r\n",
        "        context = para[\"context\"]\r\n",
        "        for qa in para[\"qas\"]:\r\n",
        "          question = qa[\"question\"]\r\n",
        "          if \"answers\" in qa:\r\n",
        "            answer_text = qa[\"answers\"][0][\"text\"]\r\n",
        "            all_answers = [_[\"text\"] for _ in qa[\"answers\"]]\r\n",
        "            start_char_idx = qa[\"answers\"][0][\"answers_start\"]\r\n",
        "            squad_eg = Sample(question, context, start_char_idx, answer_text, all_answers)\r\n",
        "          else:\r\n",
        "            squad_eg = Sample(question, context)\r\n",
        "          squad_eg.preprocess()\r\n",
        "          squad_examples.append(squad_eg)\r\n",
        "    return squad_examples"
      ],
      "execution_count": 45,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yufi_yt-Gfr0"
      },
      "source": [
        "def create_inputs_targets(squad_examples):\r\n",
        "  dataset_dict = {\r\n",
        "      \"input_word_ids\": [],\r\n",
        "      \"input_type_ids\": [],\r\n",
        "      \"input_mask\": [],\r\n",
        "      \"start_token_idx\": [],\r\n",
        "      \"end_token_idx\": []\r\n",
        "  }\r\n",
        "  for item in squad_examples:\r\n",
        "    if item.skip == False:\r\n",
        "      for key in dataset_dict:\r\n",
        "        dataset_dict[key].append(getattr(item, key))\r\n",
        "  \r\n",
        "  for key in dataset_dict:\r\n",
        "    dataset_dict[key] = np.array(dataset_dict[key])\r\n",
        "  x = [dataset_dict[\"input_word_ids\"],\r\n",
        "       dataset_dict[\"input_mask\"],\r\n",
        "       dataset_dict[\"input_type_ids\"]]\r\n",
        "  y = [dataset_dict[\"start_token_idx\"], \r\n",
        "       dataset_dict[\"end_token_idx\"]]\r\n",
        "  return x, y"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NJAHEW_TIFWa"
      },
      "source": [
        "class ValidationCallback(keras.callbacks.Callback):\r\n",
        "  \r\n",
        "  def normalize_text(self, text):\r\n",
        "    # convert to lower case\r\n",
        "    text = text.lower()\r\n",
        "    # remove redundant characters\r\n",
        "    text = \"\".join(ch for ch in text if ch not in set(string.punctuation))\r\n",
        "    # remove articles\r\n",
        "    regex = re.compile(r\"\\b(a|an|the)\\b\", re.UNICODE)\r\n",
        "    text = re.sub(regex, \" \", text)\r\n",
        "    text = \" \".join(text.split())\r\n",
        "    return text\r\n",
        "  \r\n",
        "  def __init__(self, x_eval, y_eval):\r\n",
        "    self.x_eval = x_eval\r\n",
        "    self.y_eval = y_eval\r\n",
        "  \r\n",
        "  def on_epoch_end(self, epoch, logs=None):\r\n",
        "    pred_start, pred_end = self.model.predict(self.x_eval)\r\n",
        "    count = 0\r\n",
        "    eval_examples_no_skip = [_ for _ in eval_squad_examples if _.skip == False]\r\n",
        "    # for every pair of offsets\r\n",
        "    for idx, (start, end) in enumerate(zip(pred_start, pred_end)):\r\n",
        "      # take the required Sample object with the ground-truth answers in it\r\n",
        "      squad_eg = eval_examples_no_skip[idx]\r\n",
        "      # use offsets to get back the span of text corresponding to our predicted\r\n",
        "      # first and last tokens\r\n",
        "      offsets = squad_eg.context_token_to_char\r\n",
        "      start = np.argmax(start)\r\n",
        "      end = np.argmax(end)\r\n",
        "      if start >= len(offsets):\r\n",
        "        continue\r\n",
        "      pred_char_start = offsets[start][0]\r\n",
        "      if end < len(offsets):\r\n",
        "        pred_char_end = offsets[end][1]\r\n",
        "        pred_ans = squad_eg.context[pred_char_start:pred_char_end]\r\n",
        "      else:\r\n",
        "        pred_ans = squad_eg.context[pred_char_start:]\r\n",
        "      \r\n",
        "      normalized_pred_ans = self.normalize_text(pred_ans)\r\n",
        "      # clean the real answers\r\n",
        "      normalized_true_ans = [self.normalize_text(_) for _ in squad_eg.all_answers]\r\n",
        "      # check if the predicted answer is in an array of the ground-truth answers\r\n",
        "      if normalized_pred_ans in normalized_true_ans:\r\n",
        "        count += 1\r\n",
        "    acc = count / len(self.y_eval[0])\r\n",
        "    print(f\"\\nepoch={epoch + 1}, exact match score={acc:.2f}\")    "
      ],
      "execution_count": 60,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ovpD8XkfVpTU"
      },
      "source": [
        "#3.Download a BERT model and fine-tune it"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "m61IJ0VrVvPw"
      },
      "source": [
        "bert_layer = hub.KerasLayer(\"https://tfhub.dev/tensorflow/bert_en_uncased_L-12_H-768_A-12/2\", trainable=True)\r\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H2cYcF0_Vvxv"
      },
      "source": [
        "#4.Evaluate the QA model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IoywxGkzVxO3"
      },
      "source": [
        "def get_span_from_scores(start_scores, end_scores, input_mask, verbose=False):\r\n",
        "    \"\"\"\r\n",
        "    Find start and end indices that maximize sum of start score\r\n",
        "    and end score, subject to the constraint that start is before end\r\n",
        "    and both are valid according to input_mask.\r\n",
        "\r\n",
        "    Args:\r\n",
        "        start_scores (list): contains scores for start positions, shape (1, n)\r\n",
        "        end_scores (list): constains scores for end positions, shape (1, n)\r\n",
        "        input_mask (list): 1 for valid positions and 0 otherwise\r\n",
        "    \"\"\"\r\n",
        "    n = len(start_scores)\r\n",
        "    max_start_i = -1\r\n",
        "    max_end_j = -1\r\n",
        "    max_start_score = -np.inf\r\n",
        "    max_end_score = -np.inf\r\n",
        "    max_sum = -np.inf\r\n",
        "    \r\n",
        "    # Find i and j that maximizes start_scores[i] + end_scores[j]\r\n",
        "    # so that i <= j and input_mask[i] == input_mask[j] == 1\r\n",
        "    \r\n",
        "    ### START CODE HERE (REPLACE INSTANCES OF 'None' with your code) ###\r\n",
        "    # set the range for i\r\n",
        "    for i in range(n): # complete this line\r\n",
        "        \r\n",
        "        # set the range for j\r\n",
        "        for j in range(i,n): #complete this line\r\n",
        "\r\n",
        "            # both input masks should be 1\r\n",
        "            if input_mask[i] == input_mask[j] == 1: # complete this line\r\n",
        "                \r\n",
        "                # check if the sum of the start and end scores is greater than the previous max sum\r\n",
        "                if start_scores[i] + end_scores[j] > max_sum: # complete this line\r\n",
        "\r\n",
        "                    # calculate the new max sum\r\n",
        "                    max_sum = start_scores[i] + end_scores[j]\r\n",
        "        \r\n",
        "                    # save the index of the max start score\r\n",
        "                    max_start_i = i\r\n",
        "                \r\n",
        "                    # save the index for the max end score\r\n",
        "                    max_end_j = j\r\n",
        "                    \r\n",
        "                    # save the value of the max start score\r\n",
        "                    max_start_val = start_scores[i]\r\n",
        "                    \r\n",
        "                    # save the value of the max end score\r\n",
        "                    max_end_val = end_scores[j]\r\n",
        "                                        \r\n",
        "    ### END CODE HERE ###\r\n",
        "    if verbose:\r\n",
        "        print(f\"max start is at index i={max_start_i} and score {max_start_val}\")\r\n",
        "        print(f\"max end is at index i={max_end_j} and score {max_end_val}\")\r\n",
        "        print(f\"max start + max end sum of scores is {max_sum}\")\r\n",
        "    return max_start_i, max_end_j"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-MWkZ00dX_9g"
      },
      "source": [
        "start_scores = tf.convert_to_tensor([-1, 2, 0.4, -0.3, 0, 8, 10, 12], dtype=float)\r\n",
        "end_scores = tf.convert_to_tensor([5, 1, 1, 3, 4, 10, 10, 10], dtype=float)\r\n",
        "input_mask = [1, 1, 1, 1, 1, 0, 0, 0]\r\n",
        "\r\n",
        "start, end = get_span_from_scores(start_scores, end_scores, input_mask, verbose=True)\r\n",
        "\r\n",
        "print(\"Expected: (1, 4) \\nReturned: ({}, {})\".format(start, end))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ffj6ze7HYA8n"
      },
      "source": [
        "# Test 2\r\n",
        "\r\n",
        "start_scores = tf.convert_to_tensor([0, 2, -1, 0.4, -0.3, 0, 8, 10, 12], dtype=float)\r\n",
        "end_scores = tf.convert_to_tensor([0, 5, 1, 1, 3, 4, 10, 10, 10], dtype=float)\r\n",
        "input_mask = [1, 1, 1, 1, 1, 0, 0, 0, 0 ]\r\n",
        "\r\n",
        "start, end = get_span_from_scores(start_scores, end_scores, input_mask, verbose=True)\r\n",
        "\r\n",
        "print(\"Expected: (1, 1) \\nReturned: ({}, {})\".format(start, end))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LAY_eV2iYCiz"
      },
      "source": [
        "# UNQ_C5 (UNIQUE CELL IDENTIFIER, DO NOT EDIT)\r\n",
        "def construct_answer(tokens):\r\n",
        "    \"\"\"\r\n",
        "    Combine tokens into a string, remove some hash symbols, and leading/trailing whitespace.\r\n",
        "    Args:\r\n",
        "        tokens: a list of tokens (strings)\r\n",
        "    \r\n",
        "    Returns:\r\n",
        "        out_string: the processed string.\r\n",
        "    \"\"\"\r\n",
        "    \r\n",
        "    ### START CODE HERE (REPLACE INSTANCES OF 'None' with your code) ###\r\n",
        "    \r\n",
        "    # join the tokens together with whitespace\r\n",
        "    out_string = \" \".join(tokens)\r\n",
        "    \r\n",
        "    # replace ' ##' with empty string\r\n",
        "    out_string = out_string.replace(' ##','')\r\n",
        "    \r\n",
        "    # remove leading and trailing whitespace\r\n",
        "    out_string = out_string.strip()\r\n",
        "\r\n",
        "    ### END CODE HERE ###\r\n",
        "    \r\n",
        "    # if there is an '@' symbol in the tokens, remove all whitespace\r\n",
        "    if '@' in tokens:\r\n",
        "        out_string = out_string.replace(' ', '')\r\n",
        "\r\n",
        "    return out_string"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8QHvFNh7YEKW"
      },
      "source": [
        "# Test\r\n",
        "\r\n",
        "tmp_tokens_1 = [' ## hello', 'how ', 'are ', 'you?      ']\r\n",
        "tmp_out_string_1 = construct_answer(tmp_tokens_1)\r\n",
        "\r\n",
        "print(f\"tmp_out_string_1: {tmp_out_string_1}, length {len(tmp_out_string_1)}\")\r\n",
        "\r\n",
        "\r\n",
        "tmp_tokens_2 = ['@',' ## hello', 'how ', 'are ', 'you?      ']\r\n",
        "tmp_out_string_2 = construct_answer(tmp_tokens_2)\r\n",
        "print(f\"tmp_out_string_2: {tmp_out_string_2}, length {len(tmp_out_string_2)}\")\r\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xqsEHqCuYGpd"
      },
      "source": [
        "def get_model_answer(model, question, passage, tokenizer, max_seq_length=384):\r\n",
        "    \"\"\"\r\n",
        "    Identify answer in passage for a given question using BERT. \r\n",
        "\r\n",
        "    Args:\r\n",
        "        model (Model): pretrained Bert model which we'll use to answer questions\r\n",
        "        question (string): question string\r\n",
        "        passage (string): passage string\r\n",
        "        tokenizer (Tokenizer): used for preprocessing of input\r\n",
        "        max_seq_length (int): length of input for model\r\n",
        "        \r\n",
        "    Returns:\r\n",
        "        answer (string): answer to input question according to model\r\n",
        "    \"\"\" \r\n",
        "    # prepare input: use the function prepare_bert_input\r\n",
        "    input_ids, input_mask, tokens = prepare_bert_input(question, passage, tokenizer, max_seq_length)\r\n",
        "    \r\n",
        "    # get scores for start of answer and end of answer\r\n",
        "    # use the model returned by TFAutoModelForQuestionAnswering.from_pretrained(\"./models\")\r\n",
        "    # pass in in the input ids that are returned by prepare_bert_input\r\n",
        "    start_scores, end_scores = model(input_ids)\r\n",
        "    \r\n",
        "    # start_scores and end_scores will be tensors of shape [1,max_seq_length]\r\n",
        "    # To pass these into get_span_from_scores function, \r\n",
        "    # take the value at index 0 to get a tensor of shape [max_seq_length]\r\n",
        "    start_scores = start_scores[0]\r\n",
        "    end_scores = end_scores[0]\r\n",
        "    \r\n",
        "    # using scores, get most likely answer\r\n",
        "    # use the get_span_from_scores function\r\n",
        "    span_start, span_end = get_span_from_scores(start_scores, end_scores, input_mask)\r\n",
        "    \r\n",
        "    # Using array indexing to get the tokens from the span start to span end (including the span_end)\r\n",
        "    answer_tokens = tokens[span_start:span_end+1]\r\n",
        "    \r\n",
        "    # Combine the tokens into a single string and perform post-processing\r\n",
        "    # use construct_answer\r\n",
        "    answer = construct_answer(answer_tokens)\r\n",
        "    \r\n",
        "    return answer"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FZavI0NGYISI"
      },
      "source": [
        "passage = \"Computational complexity theory is a branch of the theory \\\r\n",
        "           of computation in theoretical computer science that focuses \\\r\n",
        "           on classifying computational problems according to their inherent \\\r\n",
        "           difficulty, and relating those classes to each other. A computational \\\r\n",
        "           problem is understood to be a task that is in principle amenable to \\\r\n",
        "           being solved by a computer, which is equivalent to stating that the \\\r\n",
        "           problem may be solved by mechanical application of mathematical steps, \\\r\n",
        "           such as an algorithm.\"\r\n",
        "\r\n",
        "question = \"What branch of theoretical computer science deals with broadly \\\r\n",
        "            classifying computational problems by difficulty and class of relationship?\"\r\n",
        "\r\n",
        "print(\"Output: {}\".format(get_model_answer(model, question, passage, tokenizer)))\r\n",
        "print(\"Expected: Computational complexity theory\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nGvk9SiYYJaE"
      },
      "source": [
        "passage = \"The word pharmacy is derived from its root word pharma which was a term used since \\\r\n",
        "           the 15th–17th centuries. However, the original Greek roots from pharmakos imply sorcery \\\r\n",
        "           or even poison. In addition to pharma responsibilities, the pharma offered general medical \\\r\n",
        "           advice and a range of services that are now performed solely by other specialist practitioners, \\\r\n",
        "           such as surgery and midwifery. The pharma (as it was referred to) often operated through a \\\r\n",
        "           retail shop which, in addition to ingredients for medicines, sold tobacco and patent medicines. \\\r\n",
        "           Often the place that did this was called an apothecary and several languages have this as the \\\r\n",
        "           dominant term, though their practices are more akin to a modern pharmacy, in English the term \\\r\n",
        "           apothecary would today be seen as outdated or only approproriate if herbal remedies were on offer \\\r\n",
        "           to a large extent. The pharmas also used many other herbs not listed. The Greek word Pharmakeia \\\r\n",
        "           (Greek: φαρμακεία) derives from pharmakon (φάρμακον), meaning 'drug', 'medicine' (or 'poison').\"\r\n",
        "\r\n",
        "question = \"What word is the word pharmacy taken from?\"\r\n",
        "\r\n",
        "print(\"Output: {}\".format(get_model_answer(model, question, passage, tokenizer)))\r\n",
        "print(\"Expected: pharma\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "k-l6i30iYKy4"
      },
      "source": [
        "passage = \"Abnormal echocardiogram findings and followup. Shortness of breath, congestive heart failure, \\\r\n",
        "           and valvular insufficiency. The patient complains of shortness of breath, which is worsening. \\\r\n",
        "           The patient underwent an echocardiogram, which shows severe mitral regurgitation and also large \\\r\n",
        "           pleural effusion. The patient is an 86-year-old female admitted for evaluation of abdominal pain \\\r\n",
        "           and bloody stools. The patient has colitis and also diverticulitis, undergoing treatment. \\\r\n",
        "           During the hospitalization, the patient complains of shortness of breath, which is worsening. \\\r\n",
        "           The patient underwent an echocardiogram, which shows severe mitral regurgitation and also large \\\r\n",
        "           pleural effusion. This consultation is for further evaluation in this regard. As per the patient, \\\r\n",
        "           she is an 86-year-old female, has limited activity level. She has been having shortness of breath \\\r\n",
        "           for many years. She also was told that she has a heart murmur, which was not followed through \\\r\n",
        "           on a regular basis.\"\r\n",
        "\r\n",
        "q1 = \"How old is the patient?\"\r\n",
        "q2 = \"Does the patient have any complaints?\"\r\n",
        "q3 = \"What is the reason for this consultation?\"\r\n",
        "q4 = \"What does her echocardiogram show?\"\r\n",
        "q5 = \"What other symptoms does the patient have?\"\r\n",
        "q6 = \"What is the gender of this patient?\"\r\n",
        "\r\n",
        "questions = [q6]\r\n",
        "\r\n",
        "for i, q in enumerate(questions):\r\n",
        "    print(\"Question {}: {}\".format(i+1, q))\r\n",
        "    print()\r\n",
        "    print(\"Answer: {}\".format(get_model_answer(model, q, passage, tokenizer)))\r\n",
        "    print()\r\n",
        "    print()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CkHR8UBXVxyv"
      },
      "source": [
        "#5.Deploy the model on Django"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wZO4HHHKV1UO"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}