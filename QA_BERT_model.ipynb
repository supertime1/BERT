{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "QA BERT model.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyOtP7HhsjcGGeRoxRB1tKT+",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/supertime1/BERT/blob/main/QA_BERT_model.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xz4W_-pMVJsX"
      },
      "source": [
        "#1.Import Dependencies"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "orAMKxKkU_Vh"
      },
      "source": [
        "import os\r\n",
        "import tensorflow.keras as keras\r\n",
        "import tensorflow as tf\r\n",
        "import tensorflow_hub as hub\r\n",
        "import tensorflow_datasets as tfds\r\n",
        "import tensorflow_text as text  # A dependency of the preprocessing model\r\n",
        "import tensorflow_addons as tfa\r\n",
        "from official.nlp import optimization\r\n",
        "import numpy as np\r\n",
        "import json\r\n",
        "\r\n",
        "tf.get_logger().setLevel('ERROR')"
      ],
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8xr0VeYgYyKp"
      },
      "source": [
        "import sys\r\n",
        "sys.path.append(r'C:\\Users\\57lzhang.US04WW4008\\Documents\\GitHub\\BERT')"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Zlt53u2UVlb-"
      },
      "source": [
        "#2.Preprocess the input data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zLm-t6NFFBI7"
      },
      "source": [
        "#use the tensorflow BERT model inputs instead of Transformer"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_2Ko_XCSfGRv"
      },
      "source": [
        "import json\r\n",
        "from pathlib import Path\r\n",
        "\r\n",
        "def read_squad(path):\r\n",
        "    path = Path(path)\r\n",
        "    with open(path, 'rb') as f:\r\n",
        "        squad_dict = json.load(f)\r\n",
        "\r\n",
        "    contexts = []\r\n",
        "    questions = []\r\n",
        "    answers = []\r\n",
        "    for group in squad_dict['data']:\r\n",
        "        for passage in group['paragraphs']:\r\n",
        "            context = passage['context']\r\n",
        "            for qa in passage['qas']:\r\n",
        "                question = qa['question']\r\n",
        "                for answer in qa['answers']:\r\n",
        "                    contexts.append(context)\r\n",
        "                    questions.append(question)\r\n",
        "                    answers.append(answer)\r\n",
        "\r\n",
        "    return contexts, questions, answers\r\n",
        "\r\n",
        "train_contexts, train_questions, train_answers = read_squad(r'C:\\Users\\57lzhang.US04WW4008\\Documents\\GitHub\\BERT\\dataset\\train-v2.0.json')\r\n",
        "val_contexts, val_questions, val_answers = read_squad(r'C:\\Users\\57lzhang.US04WW4008\\Documents\\GitHub\\BERT\\dataset\\dev-v2.0.json')"
      ],
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7YNrDUNnhKTL"
      },
      "source": [
        "def add_end_idx(answers, contexts):\r\n",
        "    for answer, context in zip(answers, contexts):\r\n",
        "        gold_text = answer['text']\r\n",
        "        start_idx = answer['answer_start']\r\n",
        "        end_idx = start_idx + len(gold_text)\r\n",
        "\r\n",
        "        # sometimes squad answers are off by a character or two ‚Äì fix this\r\n",
        "        if context[start_idx:end_idx] == gold_text:\r\n",
        "            answer['answer_end'] = end_idx\r\n",
        "        elif context[start_idx-1:end_idx-1] == gold_text:\r\n",
        "            answer['answer_start'] = start_idx - 1\r\n",
        "            answer['answer_end'] = end_idx - 1     # When the gold label is off by one character\r\n",
        "        elif context[start_idx-2:end_idx-2] == gold_text:\r\n",
        "            answer['answer_start'] = start_idx - 2\r\n",
        "            answer['answer_end'] = end_idx - 2     # When the gold label is off by two characters\r\n",
        "\r\n",
        "add_end_idx(train_answers, train_contexts)\r\n",
        "add_end_idx(val_answers, val_contexts)"
      ],
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nD8FvtIipPBW",
        "outputId": "2bbc141b-ec19-4d6d-d6cb-d8755b61f326"
      },
      "source": [
        "train_answers[0]"
      ],
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'text': 'in the late 1990s', 'answer_start': 269, 'answer_end': 286}"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 26
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rc0LtV2ahOsd"
      },
      "source": [
        "from transformers import DistilBertTokenizerFast\r\n",
        "tokenizer = DistilBertTokenizerFast.from_pretrained('distilbert-base-uncased')\r\n",
        "\r\n",
        "train_encodings = tokenizer(train_contexts, train_questions, truncation=True, padding=True)\r\n",
        "val_encodings = tokenizer(val_contexts, val_questions, truncation=True, padding=True)"
      ],
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DX1rS60ThqV-"
      },
      "source": [
        "def add_token_positions(encodings, answers):\r\n",
        "    start_positions = []\r\n",
        "    end_positions = []\r\n",
        "    for i in range(len(answers)):\r\n",
        "        start_positions.append(encodings.char_to_token(i, answers[i]['answer_start']))\r\n",
        "        end_positions.append(encodings.char_to_token(i, answers[i]['answer_end'] - 1))\r\n",
        "        # if None, the answer passage has been truncated\r\n",
        "        if start_positions[-1] is None:\r\n",
        "            start_positions[-1] = tokenizer.model_max_length\r\n",
        "        if end_positions[-1] is None:\r\n",
        "            end_positions[-1] = tokenizer.model_max_length\r\n",
        "    encodings.update({'start_positions': start_positions, 'end_positions': end_positions})\r\n",
        "\r\n",
        "add_token_positions(train_encodings, train_answers)\r\n",
        "add_token_positions(val_encodings, val_answers)"
      ],
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TGzQWu2vpjm_",
        "outputId": "07148c56-982d-479c-e3cc-ac642c7e75dc"
      },
      "source": [
        "train_encodings.keys()"
      ],
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "dict_keys(['input_ids', 'attention_mask', 'start_positions', 'end_positions'])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 29
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PxOJA4dUxTwf"
      },
      "source": [
        "import tensorflow as tf\r\n",
        "\r\n",
        "train_dataset = tf.data.Dataset.from_tensor_slices((\r\n",
        "    {key: train_encodings[key] for key in ['input_ids', 'attention_mask']},\r\n",
        "    {key: train_encodings[key] for key in ['start_positions', 'end_positions']}\r\n",
        "))\r\n",
        "val_dataset = tf.data.Dataset.from_tensor_slices((\r\n",
        "    {key: val_encodings[key] for key in ['input_ids', 'attention_mask']},\r\n",
        "    {key: val_encodings[key] for key in ['start_positions', 'end_positions']}\r\n",
        "))"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IDpZFK_EZ00P"
      },
      "source": [
        "from transformers import AutoModelForQuestionAnswering\r\n",
        "model = AutoModelForQuestionAnswering.from_pretrained(\"bert-large-uncased-whole-word-masking-finetuned-squad\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zm_0FuApqLlj",
        "outputId": "cd889554-ee40-410a-e5bc-9b6de6a9c834"
      },
      "source": [
        "model.summary()"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"tf_distil_bert_for_question_answering\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "distilbert (TFDistilBertMain multiple                  66362880  \n",
            "_________________________________________________________________\n",
            "qa_outputs (Dense)           multiple                  1538      \n",
            "_________________________________________________________________\n",
            "dropout_19 (Dropout)         multiple                  0         \n",
            "=================================================================\n",
            "Total params: 66,364,418\n",
            "Trainable params: 66,364,418\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "CG3v1W1QbSTh",
        "outputId": "3e5c0d8d-02bd-444b-e96d-c623b8cc4008"
      },
      "source": [
        "# Keras will expect a tuple when dealing with labels\r\n",
        "train_dataset = train_dataset.map(lambda x, y: (x, (y['start_positions'], y['end_positions'])))\r\n",
        "\r\n",
        "# Keras will assign a separate loss for each output and add them together. So we'll just use the standard CE loss\r\n",
        "# instead of using the built-in model.compute_loss, which expects a dict of outputs and averages the two terms.\r\n",
        "# Note that this means the loss will be 2x of when using TFTrainer since we're adding instead of averaging them.\r\n",
        "loss = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)\r\n",
        "model.distilbert.return_dict = False # if using ü§ó Transformers >3.02, make sure outputs are tuples\r\n",
        "\r\n",
        "optimizer = tf.keras.optimizers.Adam(learning_rate=5e-5)\r\n",
        "model.compile(optimizer=optimizer, loss=loss) # can also use any keras loss fn\r\n",
        "model.fit(train_dataset.shuffle(1000).batch(16), epochs=3, batch_size=16)"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/3\n",
            "WARNING:tensorflow:AutoGraph could not transform <bound method StreamWrapper.write of <colorama.ansitowin32.StreamWrapper object at 0x0000022D7A37B648>> and will run it as-is.\n",
            "Cause: mangled names are not yet supported\n",
            "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
            "WARNING: AutoGraph could not transform <bound method StreamWrapper.write of <colorama.ansitowin32.StreamWrapper object at 0x0000022D7A37B648>> and will run it as-is.\n",
            "Cause: mangled names are not yet supported\n",
            "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "The parameters `output_attentions`, `output_hidden_states` and `use_cache` cannot be updated when calling a model.They have to be set to True/False in the config object (i.e.: `config=XConfig.from_pretrained('name', output_attentions=True)`).\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:AutoGraph could not transform <bound method Socket.send of <zmq.sugar.socket.Socket object at 0x0000022D73D739E8>> and will run it as-is.\n",
            "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
            "Cause: module, class, method, function, traceback, frame, or code object was expected, got cython_function_or_method\n",
            "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
            "WARNING: AutoGraph could not transform <bound method Socket.send of <zmq.sugar.socket.Socket object at 0x0000022D73D739E8>> and will run it as-is.\n",
            "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
            "Cause: module, class, method, function, traceback, frame, or code object was expected, got cython_function_or_method\n",
            "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "The parameter `return_dict` cannot be set in graph mode and will always be set to `True`.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "error",
          "ename": "TypeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[1;32m<ipython-input-10-6824975d2092>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     10\u001b[0m \u001b[0moptimizer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mkeras\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moptimizers\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mAdam\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlearning_rate\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m5e-5\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     11\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcompile\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mloss\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mloss\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;31m# can also use any keras loss fn\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 12\u001b[1;33m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrain_dataset\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshuffle\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m1000\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbatch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m16\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m3\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m16\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[0;32m   1098\u001b[0m                 _r=1):\n\u001b[0;32m   1099\u001b[0m               \u001b[0mcallbacks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mon_train_batch_begin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1100\u001b[1;33m               \u001b[0mtmp_logs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrain_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1101\u001b[0m               \u001b[1;32mif\u001b[0m \u001b[0mdata_handler\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshould_sync\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1102\u001b[0m                 \u001b[0mcontext\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0masync_wait\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
            "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\eager\\def_function.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    826\u001b[0m     \u001b[0mtracing_count\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mexperimental_get_tracing_count\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    827\u001b[0m     \u001b[1;32mwith\u001b[0m \u001b[0mtrace\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTrace\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_name\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mtm\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 828\u001b[1;33m       \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    829\u001b[0m       \u001b[0mcompiler\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m\"xla\"\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_experimental_compile\u001b[0m \u001b[1;32melse\u001b[0m \u001b[1;34m\"nonXla\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    830\u001b[0m       \u001b[0mnew_tracing_count\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mexperimental_get_tracing_count\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
            "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\eager\\def_function.py\u001b[0m in \u001b[0;36m_call\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    869\u001b[0m       \u001b[1;31m# This is the first call of __call__, so we have to initialize.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    870\u001b[0m       \u001b[0minitializers\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 871\u001b[1;33m       \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_initialize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0madd_initializers_to\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0minitializers\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    872\u001b[0m     \u001b[1;32mfinally\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    873\u001b[0m       \u001b[1;31m# At this point we know that the initialization is complete (or less\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
            "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\eager\\def_function.py\u001b[0m in \u001b[0;36m_initialize\u001b[1;34m(self, args, kwds, add_initializers_to)\u001b[0m\n\u001b[0;32m    724\u001b[0m     self._concrete_stateful_fn = (\n\u001b[0;32m    725\u001b[0m         self._stateful_fn._get_concrete_function_internal_garbage_collected(  # pylint: disable=protected-access\n\u001b[1;32m--> 726\u001b[1;33m             *args, **kwds))\n\u001b[0m\u001b[0;32m    727\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    728\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0minvalid_creator_scope\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0munused_args\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0munused_kwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
            "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\eager\\function.py\u001b[0m in \u001b[0;36m_get_concrete_function_internal_garbage_collected\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   2967\u001b[0m       \u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwargs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2968\u001b[0m     \u001b[1;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_lock\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2969\u001b[1;33m       \u001b[0mgraph_function\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0m_\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_maybe_define_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2970\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mgraph_function\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2971\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
            "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\eager\\function.py\u001b[0m in \u001b[0;36m_maybe_define_function\u001b[1;34m(self, args, kwargs)\u001b[0m\n\u001b[0;32m   3359\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3360\u001b[0m           \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_function_cache\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmissed\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0madd\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcall_context_key\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 3361\u001b[1;33m           \u001b[0mgraph_function\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_create_graph_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   3362\u001b[0m           \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_function_cache\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mprimary\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mcache_key\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mgraph_function\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3363\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
            "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\eager\\function.py\u001b[0m in \u001b[0;36m_create_graph_function\u001b[1;34m(self, args, kwargs, override_flat_arg_shapes)\u001b[0m\n\u001b[0;32m   3204\u001b[0m             \u001b[0marg_names\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0marg_names\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3205\u001b[0m             \u001b[0moverride_flat_arg_shapes\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0moverride_flat_arg_shapes\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 3206\u001b[1;33m             capture_by_value=self._capture_by_value),\n\u001b[0m\u001b[0;32m   3207\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_function_attributes\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3208\u001b[0m         \u001b[0mfunction_spec\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfunction_spec\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
            "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\func_graph.py\u001b[0m in \u001b[0;36mfunc_graph_from_py_func\u001b[1;34m(name, python_func, args, kwargs, signature, func_graph, autograph, autograph_options, add_control_dependencies, arg_names, op_return_value, collections, capture_by_value, override_flat_arg_shapes)\u001b[0m\n\u001b[0;32m    988\u001b[0m         \u001b[0m_\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moriginal_func\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf_decorator\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0munwrap\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpython_func\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    989\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 990\u001b[1;33m       \u001b[0mfunc_outputs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpython_func\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0mfunc_args\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mfunc_kwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    991\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    992\u001b[0m       \u001b[1;31m# invariant: `func_outputs` contains only Tensors, CompositeTensors,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
            "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\eager\\def_function.py\u001b[0m in \u001b[0;36mwrapped_fn\u001b[1;34m(*args, **kwds)\u001b[0m\n\u001b[0;32m    632\u001b[0m             \u001b[0mxla_context\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mExit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    633\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 634\u001b[1;33m           \u001b[0mout\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mweak_wrapped_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__wrapped__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    635\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mout\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    636\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
            "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\func_graph.py\u001b[0m in \u001b[0;36mwrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    975\u001b[0m           \u001b[1;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m  \u001b[1;31m# pylint:disable=broad-except\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    976\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"ag_error_metadata\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 977\u001b[1;33m               \u001b[1;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mag_error_metadata\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mto_exception\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    978\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    979\u001b[0m               \u001b[1;32mraise\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
            "\u001b[1;31mTypeError\u001b[0m: in user code:\n\n    C:\\Users\\57lzhang.US04WW4008\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\training.py:805 train_function  *\n        return step_function(self, iterator)\n    C:\\Users\\57lzhang.US04WW4008\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\training.py:795 step_function  **\n        outputs = model.distribute_strategy.run(run_step, args=(data,))\n    C:\\Users\\57lzhang.US04WW4008\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\distribute\\distribute_lib.py:1259 run\n        return self._extended.call_for_each_replica(fn, args=args, kwargs=kwargs)\n    C:\\Users\\57lzhang.US04WW4008\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\distribute\\distribute_lib.py:2730 call_for_each_replica\n        return self._call_for_each_replica(fn, args, kwargs)\n    C:\\Users\\57lzhang.US04WW4008\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\distribute\\distribute_lib.py:3417 _call_for_each_replica\n        return fn(*args, **kwargs)\n    C:\\Users\\57lzhang.US04WW4008\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\training.py:788 run_step  **\n        outputs = model.train_step(data)\n    C:\\Users\\57lzhang.US04WW4008\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\training.py:758 train_step\n        self.compiled_metrics.update_state(y, y_pred, sample_weight)\n    C:\\Users\\57lzhang.US04WW4008\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\compile_utils.py:387 update_state\n        self.build(y_pred, y_true)\n    C:\\Users\\57lzhang.US04WW4008\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\compile_utils.py:318 build\n        self._metrics, y_true, y_pred)\n    C:\\Users\\57lzhang.US04WW4008\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\util\\nest.py:1163 map_structure_up_to\n        **kwargs)\n    C:\\Users\\57lzhang.US04WW4008\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\util\\nest.py:1245 map_structure_with_tuple_paths_up_to\n        expand_composites=expand_composites)\n    C:\\Users\\57lzhang.US04WW4008\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\util\\nest.py:849 assert_shallow_structure\n        shallow_type=type(shallow_tree)))\n\n    TypeError: The two structures don't have the same sequence type. Input structure has type <class 'tuple'>, while shallow structure has type <class 'transformers.modeling_tf_outputs.TFQuestionAnsweringModelOutput'>.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ovpD8XkfVpTU"
      },
      "source": [
        "#3.Download a BERT model and fine-tune it"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "m61IJ0VrVvPw"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H2cYcF0_Vvxv"
      },
      "source": [
        "#4.Evaluate the QA model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IoywxGkzVxO3"
      },
      "source": [
        "def get_span_from_scores(start_scores, end_scores, input_mask, verbose=False):\r\n",
        "    \"\"\"\r\n",
        "    Find start and end indices that maximize sum of start score\r\n",
        "    and end score, subject to the constraint that start is before end\r\n",
        "    and both are valid according to input_mask.\r\n",
        "\r\n",
        "    Args:\r\n",
        "        start_scores (list): contains scores for start positions, shape (1, n)\r\n",
        "        end_scores (list): constains scores for end positions, shape (1, n)\r\n",
        "        input_mask (list): 1 for valid positions and 0 otherwise\r\n",
        "    \"\"\"\r\n",
        "    n = len(start_scores)\r\n",
        "    max_start_i = -1\r\n",
        "    max_end_j = -1\r\n",
        "    max_start_score = -np.inf\r\n",
        "    max_end_score = -np.inf\r\n",
        "    max_sum = -np.inf\r\n",
        "    \r\n",
        "    # Find i and j that maximizes start_scores[i] + end_scores[j]\r\n",
        "    # so that i <= j and input_mask[i] == input_mask[j] == 1\r\n",
        "    \r\n",
        "    ### START CODE HERE (REPLACE INSTANCES OF 'None' with your code) ###\r\n",
        "    # set the range for i\r\n",
        "    for i in range(n): # complete this line\r\n",
        "        \r\n",
        "        # set the range for j\r\n",
        "        for j in range(i,n): #complete this line\r\n",
        "\r\n",
        "            # both input masks should be 1\r\n",
        "            if input_mask[i] == input_mask[j] == 1: # complete this line\r\n",
        "                \r\n",
        "                # check if the sum of the start and end scores is greater than the previous max sum\r\n",
        "                if start_scores[i] + end_scores[j] > max_sum: # complete this line\r\n",
        "\r\n",
        "                    # calculate the new max sum\r\n",
        "                    max_sum = start_scores[i] + end_scores[j]\r\n",
        "        \r\n",
        "                    # save the index of the max start score\r\n",
        "                    max_start_i = i\r\n",
        "                \r\n",
        "                    # save the index for the max end score\r\n",
        "                    max_end_j = j\r\n",
        "                    \r\n",
        "                    # save the value of the max start score\r\n",
        "                    max_start_val = start_scores[i]\r\n",
        "                    \r\n",
        "                    # save the value of the max end score\r\n",
        "                    max_end_val = end_scores[j]\r\n",
        "                                        \r\n",
        "    ### END CODE HERE ###\r\n",
        "    if verbose:\r\n",
        "        print(f\"max start is at index i={max_start_i} and score {max_start_val}\")\r\n",
        "        print(f\"max end is at index i={max_end_j} and score {max_end_val}\")\r\n",
        "        print(f\"max start + max end sum of scores is {max_sum}\")\r\n",
        "    return max_start_i, max_end_j"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-MWkZ00dX_9g"
      },
      "source": [
        "start_scores = tf.convert_to_tensor([-1, 2, 0.4, -0.3, 0, 8, 10, 12], dtype=float)\r\n",
        "end_scores = tf.convert_to_tensor([5, 1, 1, 3, 4, 10, 10, 10], dtype=float)\r\n",
        "input_mask = [1, 1, 1, 1, 1, 0, 0, 0]\r\n",
        "\r\n",
        "start, end = get_span_from_scores(start_scores, end_scores, input_mask, verbose=True)\r\n",
        "\r\n",
        "print(\"Expected: (1, 4) \\nReturned: ({}, {})\".format(start, end))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ffj6ze7HYA8n"
      },
      "source": [
        "# Test 2\r\n",
        "\r\n",
        "start_scores = tf.convert_to_tensor([0, 2, -1, 0.4, -0.3, 0, 8, 10, 12], dtype=float)\r\n",
        "end_scores = tf.convert_to_tensor([0, 5, 1, 1, 3, 4, 10, 10, 10], dtype=float)\r\n",
        "input_mask = [1, 1, 1, 1, 1, 0, 0, 0, 0 ]\r\n",
        "\r\n",
        "start, end = get_span_from_scores(start_scores, end_scores, input_mask, verbose=True)\r\n",
        "\r\n",
        "print(\"Expected: (1, 1) \\nReturned: ({}, {})\".format(start, end))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LAY_eV2iYCiz"
      },
      "source": [
        "# UNQ_C5 (UNIQUE CELL IDENTIFIER, DO NOT EDIT)\r\n",
        "def construct_answer(tokens):\r\n",
        "    \"\"\"\r\n",
        "    Combine tokens into a string, remove some hash symbols, and leading/trailing whitespace.\r\n",
        "    Args:\r\n",
        "        tokens: a list of tokens (strings)\r\n",
        "    \r\n",
        "    Returns:\r\n",
        "        out_string: the processed string.\r\n",
        "    \"\"\"\r\n",
        "    \r\n",
        "    ### START CODE HERE (REPLACE INSTANCES OF 'None' with your code) ###\r\n",
        "    \r\n",
        "    # join the tokens together with whitespace\r\n",
        "    out_string = \" \".join(tokens)\r\n",
        "    \r\n",
        "    # replace ' ##' with empty string\r\n",
        "    out_string = out_string.replace(' ##','')\r\n",
        "    \r\n",
        "    # remove leading and trailing whitespace\r\n",
        "    out_string = out_string.strip()\r\n",
        "\r\n",
        "    ### END CODE HERE ###\r\n",
        "    \r\n",
        "    # if there is an '@' symbol in the tokens, remove all whitespace\r\n",
        "    if '@' in tokens:\r\n",
        "        out_string = out_string.replace(' ', '')\r\n",
        "\r\n",
        "    return out_string"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8QHvFNh7YEKW"
      },
      "source": [
        "# Test\r\n",
        "\r\n",
        "tmp_tokens_1 = [' ## hello', 'how ', 'are ', 'you?      ']\r\n",
        "tmp_out_string_1 = construct_answer(tmp_tokens_1)\r\n",
        "\r\n",
        "print(f\"tmp_out_string_1: {tmp_out_string_1}, length {len(tmp_out_string_1)}\")\r\n",
        "\r\n",
        "\r\n",
        "tmp_tokens_2 = ['@',' ## hello', 'how ', 'are ', 'you?      ']\r\n",
        "tmp_out_string_2 = construct_answer(tmp_tokens_2)\r\n",
        "print(f\"tmp_out_string_2: {tmp_out_string_2}, length {len(tmp_out_string_2)}\")\r\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xqsEHqCuYGpd"
      },
      "source": [
        "def get_model_answer(model, question, passage, tokenizer, max_seq_length=384):\r\n",
        "    \"\"\"\r\n",
        "    Identify answer in passage for a given question using BERT. \r\n",
        "\r\n",
        "    Args:\r\n",
        "        model (Model): pretrained Bert model which we'll use to answer questions\r\n",
        "        question (string): question string\r\n",
        "        passage (string): passage string\r\n",
        "        tokenizer (Tokenizer): used for preprocessing of input\r\n",
        "        max_seq_length (int): length of input for model\r\n",
        "        \r\n",
        "    Returns:\r\n",
        "        answer (string): answer to input question according to model\r\n",
        "    \"\"\" \r\n",
        "    # prepare input: use the function prepare_bert_input\r\n",
        "    input_ids, input_mask, tokens = prepare_bert_input(question, passage, tokenizer, max_seq_length)\r\n",
        "    \r\n",
        "    # get scores for start of answer and end of answer\r\n",
        "    # use the model returned by TFAutoModelForQuestionAnswering.from_pretrained(\"./models\")\r\n",
        "    # pass in in the input ids that are returned by prepare_bert_input\r\n",
        "    start_scores, end_scores = model(input_ids)\r\n",
        "    \r\n",
        "    # start_scores and end_scores will be tensors of shape [1,max_seq_length]\r\n",
        "    # To pass these into get_span_from_scores function, \r\n",
        "    # take the value at index 0 to get a tensor of shape [max_seq_length]\r\n",
        "    start_scores = start_scores[0]\r\n",
        "    end_scores = end_scores[0]\r\n",
        "    \r\n",
        "    # using scores, get most likely answer\r\n",
        "    # use the get_span_from_scores function\r\n",
        "    span_start, span_end = get_span_from_scores(start_scores, end_scores, input_mask)\r\n",
        "    \r\n",
        "    # Using array indexing to get the tokens from the span start to span end (including the span_end)\r\n",
        "    answer_tokens = tokens[span_start:span_end+1]\r\n",
        "    \r\n",
        "    # Combine the tokens into a single string and perform post-processing\r\n",
        "    # use construct_answer\r\n",
        "    answer = construct_answer(answer_tokens)\r\n",
        "    \r\n",
        "    return answer"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FZavI0NGYISI"
      },
      "source": [
        "passage = \"Computational complexity theory is a branch of the theory \\\r\n",
        "           of computation in theoretical computer science that focuses \\\r\n",
        "           on classifying computational problems according to their inherent \\\r\n",
        "           difficulty, and relating those classes to each other. A computational \\\r\n",
        "           problem is understood to be a task that is in principle amenable to \\\r\n",
        "           being solved by a computer, which is equivalent to stating that the \\\r\n",
        "           problem may be solved by mechanical application of mathematical steps, \\\r\n",
        "           such as an algorithm.\"\r\n",
        "\r\n",
        "question = \"What branch of theoretical computer science deals with broadly \\\r\n",
        "            classifying computational problems by difficulty and class of relationship?\"\r\n",
        "\r\n",
        "print(\"Output: {}\".format(get_model_answer(model, question, passage, tokenizer)))\r\n",
        "print(\"Expected: Computational complexity theory\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nGvk9SiYYJaE"
      },
      "source": [
        "passage = \"The word pharmacy is derived from its root word pharma which was a term used since \\\r\n",
        "           the 15th‚Äì17th centuries. However, the original Greek roots from pharmakos imply sorcery \\\r\n",
        "           or even poison. In addition to pharma responsibilities, the pharma offered general medical \\\r\n",
        "           advice and a range of services that are now performed solely by other specialist practitioners, \\\r\n",
        "           such as surgery and midwifery. The pharma (as it was referred to) often operated through a \\\r\n",
        "           retail shop which, in addition to ingredients for medicines, sold tobacco and patent medicines. \\\r\n",
        "           Often the place that did this was called an apothecary and several languages have this as the \\\r\n",
        "           dominant term, though their practices are more akin to a modern pharmacy, in English the term \\\r\n",
        "           apothecary would today be seen as outdated or only approproriate if herbal remedies were on offer \\\r\n",
        "           to a large extent. The pharmas also used many other herbs not listed. The Greek word Pharmakeia \\\r\n",
        "           (Greek: œÜŒ±œÅŒºŒ±Œ∫ŒµŒØŒ±) derives from pharmakon (œÜŒ¨œÅŒºŒ±Œ∫ŒøŒΩ), meaning 'drug', 'medicine' (or 'poison').\"\r\n",
        "\r\n",
        "question = \"What word is the word pharmacy taken from?\"\r\n",
        "\r\n",
        "print(\"Output: {}\".format(get_model_answer(model, question, passage, tokenizer)))\r\n",
        "print(\"Expected: pharma\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "k-l6i30iYKy4"
      },
      "source": [
        "passage = \"Abnormal echocardiogram findings and followup. Shortness of breath, congestive heart failure, \\\r\n",
        "           and valvular insufficiency. The patient complains of shortness of breath, which is worsening. \\\r\n",
        "           The patient underwent an echocardiogram, which shows severe mitral regurgitation and also large \\\r\n",
        "           pleural effusion. The patient is an 86-year-old female admitted for evaluation of abdominal pain \\\r\n",
        "           and bloody stools. The patient has colitis and also diverticulitis, undergoing treatment. \\\r\n",
        "           During the hospitalization, the patient complains of shortness of breath, which is worsening. \\\r\n",
        "           The patient underwent an echocardiogram, which shows severe mitral regurgitation and also large \\\r\n",
        "           pleural effusion. This consultation is for further evaluation in this regard. As per the patient, \\\r\n",
        "           she is an 86-year-old female, has limited activity level. She has been having shortness of breath \\\r\n",
        "           for many years. She also was told that she has a heart murmur, which was not followed through \\\r\n",
        "           on a regular basis.\"\r\n",
        "\r\n",
        "q1 = \"How old is the patient?\"\r\n",
        "q2 = \"Does the patient have any complaints?\"\r\n",
        "q3 = \"What is the reason for this consultation?\"\r\n",
        "q4 = \"What does her echocardiogram show?\"\r\n",
        "q5 = \"What other symptoms does the patient have?\"\r\n",
        "q6 = \"What is the gender of this patient?\"\r\n",
        "\r\n",
        "questions = [q6]\r\n",
        "\r\n",
        "for i, q in enumerate(questions):\r\n",
        "    print(\"Question {}: {}\".format(i+1, q))\r\n",
        "    print()\r\n",
        "    print(\"Answer: {}\".format(get_model_answer(model, q, passage, tokenizer)))\r\n",
        "    print()\r\n",
        "    print()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CkHR8UBXVxyv"
      },
      "source": [
        "#5.Deploy the model on Django"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wZO4HHHKV1UO"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}