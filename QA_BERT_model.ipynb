{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "QA BERT model.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "xz4W_-pMVJsX",
        "ovpD8XkfVpTU",
        "H2cYcF0_Vvxv",
        "CkHR8UBXVxyv"
      ],
      "authorship_tag": "ABX9TyNbBGvy9jXXdrBUXNzgg5rD",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "TPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/supertime1/BERT/blob/main/QA_BERT_model.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xz4W_-pMVJsX"
      },
      "source": [
        "#1.Import Dependencies"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gsGeLLt-Z4ot"
      },
      "source": [
        "%%capture\r\n",
        "!pip install tokenizers\r\n",
        "!pip install tensorflow_text\r\n",
        "!pip install -q tf-models-official"
      ],
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "orAMKxKkU_Vh"
      },
      "source": [
        "import os\r\n",
        "import tensorflow.keras as keras\r\n",
        "import tensorflow as tf\r\n",
        "import tensorflow_hub as hub\r\n",
        "import tensorflow_datasets as tfds\r\n",
        "import tensorflow_text as text  # A dependency of the preprocessing model\r\n",
        "import tensorflow_addons as tfa\r\n",
        "from official.nlp import optimization, bert\r\n",
        "import numpy as np\r\n",
        "import json\r\n",
        "from tokenizers import BertWordPieceTokenizer\r\n",
        "import string\r\n",
        "import re\r\n",
        "tf.get_logger().setLevel('ERROR')\r\n",
        "import official.nlp.bert.tokenization"
      ],
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3I6GNqFKSkt8",
        "outputId": "018e0c02-73a2-42e6-e88c-d0ddcd0c0595"
      },
      "source": [
        "try:\r\n",
        "  tpu = tf.distribute.cluster_resolver.TPUClusterResolver()  # TPU detection\r\n",
        "  print('Running on TPU ', tpu.cluster_spec().as_dict()['worker'])\r\n",
        "except ValueError:\r\n",
        "  raise BaseException('ERROR: Not connected to a TPU runtime; please see the previous cell in this notebook for instructions!')\r\n",
        "\r\n",
        "tf.config.experimental_connect_to_cluster(tpu)\r\n",
        "tf.tpu.experimental.initialize_tpu_system(tpu)\r\n",
        "tpu_strategy = tf.distribute.experimental.TPUStrategy(tpu)"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Running on TPU  ['10.122.190.42:8470']\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:absl:`tf.distribute.experimental.TPUStrategy` is deprecated, please use  the non experimental symbol `tf.distribute.TPUStrategy` instead.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KJaLqPyo6JhW",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d270d5ba-5517-45a8-8a77-69b049249764"
      },
      "source": [
        "from tensorflow.python.client import device_lib \r\n",
        "print(device_lib.list_local_devices())"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[name: \"/device:CPU:0\"\n",
            "device_type: \"CPU\"\n",
            "memory_limit: 268435456\n",
            "locality {\n",
            "}\n",
            "incarnation: 10791792998110905201\n",
            "]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GqtflcAO6W9x",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "cfceb447-0aa4-45f5-8c55-0abccd3c3b07"
      },
      "source": [
        "tf.test.is_gpu_available(cuda_only=False, min_cuda_compute_capability=None)"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "False"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Zlt53u2UVlb-"
      },
      "source": [
        "#2.Helper functions"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kh_mG7LuzHso"
      },
      "source": [
        "class Sample:\r\n",
        "  def __init__(self, question, context, start_char_idx=None, answer_text=None, all_answers=None):\r\n",
        "    self.question = question\r\n",
        "    self.context = context\r\n",
        "    self.start_char_idx = start_char_idx\r\n",
        "    self.answer_text = answer_text\r\n",
        "    self.all_answers = all_answers\r\n",
        "    self.skip = False\r\n",
        "    self.start_token_idx = -1\r\n",
        "    self.end_token_idx = -1\r\n",
        "  \r\n",
        "  def preprocess(self):\r\n",
        "  # remove multipe whitespace between characters\r\n",
        "    context = \" \".join(str(self.context).split())\r\n",
        "    question = \" \".join(str(self.question).split())\r\n",
        "    # tokenize context and question\r\n",
        "    tokenized_context = tokenizer.encode(context)\r\n",
        "    tokenized_question = tokenizer.encode(question)\r\n",
        "    # preprocess answer if this is a validation or training sample\r\n",
        "    if self.answer_text:\r\n",
        "      answer = \" \".join(str(self.answer_text).split())\r\n",
        "      # find the end character index\r\n",
        "      end_char_idx = self.start_char_idx + len(answer)\r\n",
        "      \r\n",
        "      # sometimes squad answers are off by a character or two â€“ fix this\r\n",
        "      if context[self.start_char_idx-1:end_char_idx-1] == answer:\r\n",
        "          self.start_char_idx -= 1\r\n",
        "          end_char_idx -= 1\r\n",
        "      elif context[self.start_char_idx-2:end_char_idx-2] == answer:\r\n",
        "          self.start_char_idx -= 2\r\n",
        "          end_char_idx -= 2\r\n",
        "      \r\n",
        "      # check if end character index is in the context\r\n",
        "      if end_char_idx >= len(context):\r\n",
        "        self.skip = True\r\n",
        "        return\r\n",
        "      # mark all the character indexes in context that are also in answer\r\n",
        "      is_char_in_ans = [0] * len(context)\r\n",
        "      for idx in range(self.start_char_idx, end_char_idx):\r\n",
        "        is_char_in_ans[idx] = 1\r\n",
        "      # find all the tokens that are in the answers\r\n",
        "      ans_token_idx = []\r\n",
        "      for idx, (start, end) in enumerate(tokenized_context.offsets):\r\n",
        "        if sum(is_char_in_ans[start:end]) > 0:\r\n",
        "          ans_token_idx.append(idx)\r\n",
        "      if len(ans_token_idx) == 0:\r\n",
        "        self.skip = True\r\n",
        "        return\r\n",
        "      # get start and end token indexes\r\n",
        "      self.start_token_idx = ans_token_idx[0]\r\n",
        "      self.end_token_idx = ans_token_idx[-1]\r\n",
        "\r\n",
        "    # create inputs as usual\r\n",
        "    input_ids = tokenized_context.ids + tokenized_question.ids[1:]\r\n",
        "    attention_mask = [1] * len(input_ids)\r\n",
        "    token_type_ids = [0] * len(tokenized_context.ids) + [1] * len(tokenized_question.ids[1:])\r\n",
        "    padding_length = max_seq_length - len(input_ids)\r\n",
        "    # add padding if necessary\r\n",
        "    if padding_length > 0:\r\n",
        "      input_ids += [0] * padding_length\r\n",
        "      attention_mask += [0] * padding_length\r\n",
        "      token_type_ids += [0] * padding_length\r\n",
        "\r\n",
        "    elif padding_length < 0:\r\n",
        "      self.skip = True\r\n",
        "      return\r\n",
        "\r\n",
        "    self.input_word_ids = input_ids\r\n",
        "    self.input_type_ids = token_type_ids\r\n",
        "    self.input_mask = attention_mask\r\n",
        "    self.context_token_to_char = tokenized_context.offsets"
      ],
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XP3Lxs7CDap7"
      },
      "source": [
        "def create_squad_examples(raw_data):\r\n",
        "    squad_examples = []\r\n",
        "    for item in raw_data[\"data\"]:\r\n",
        "        for para in item[\"paragraphs\"]:\r\n",
        "            context = para[\"context\"]\r\n",
        "            for qa in para[\"qas\"]:\r\n",
        "                question = qa[\"question\"]\r\n",
        "                if qa[\"answers\"] != []:\r\n",
        "                    all_answers = [_[\"text\"] for _ in qa[\"answers\"]]\r\n",
        "                    for answer in qa['answers']:\r\n",
        "                        answer_text = answer[\"text\"]\r\n",
        "                        start_char_idx = answer[\"answer_start\"]\r\n",
        "                        squad_eg = Sample(question, context, start_char_idx, answer_text, all_answers)\r\n",
        "                else:\r\n",
        "                    squad_eg = Sample(question, context)\r\n",
        "                squad_eg.preprocess()\r\n",
        "                squad_examples.append(squad_eg)\r\n",
        "    return squad_examples"
      ],
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yufi_yt-Gfr0"
      },
      "source": [
        "def create_inputs_targets(squad_examples):\r\n",
        "    dataset_dict = {\r\n",
        "        \"input_word_ids\": [],\r\n",
        "        \"input_type_ids\": [],\r\n",
        "        \"input_mask\": [],\r\n",
        "        \"start_token_idx\": [],\r\n",
        "        \"end_token_idx\": [],\r\n",
        "    }\r\n",
        "    for item in squad_examples:\r\n",
        "        if item.skip == False:\r\n",
        "            for key in dataset_dict:\r\n",
        "                dataset_dict[key].append(getattr(item, key))\r\n",
        "    for key in dataset_dict:\r\n",
        "        dataset_dict[key] = np.array(dataset_dict[key])\r\n",
        "    x = [dataset_dict[\"input_word_ids\"],\r\n",
        "         dataset_dict[\"input_mask\"],\r\n",
        "         dataset_dict[\"input_type_ids\"]]\r\n",
        "    y = [dataset_dict[\"start_token_idx\"], dataset_dict[\"end_token_idx\"]]\r\n",
        "    return x, y"
      ],
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ovpD8XkfVpTU"
      },
      "source": [
        "#3.Download a BERT model and fine-tune it"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yoNgYLP2Zvxl"
      },
      "source": [
        "train_path = keras.utils.get_file(\"train.json\", \"https://rajpurkar.github.io/SQuAD-explorer/dataset/train-v2.0.json\")\r\n",
        "eval_path = keras.utils.get_file(\"eval.json\", \"https://rajpurkar.github.io/SQuAD-explorer/dataset/dev-v2.0.json\")\r\n",
        "with open(train_path) as f: raw_train_data = json.load(f)\r\n",
        "with open(eval_path) as f: raw_eval_data = json.load(f)"
      ],
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 352
        },
        "id": "zt6yOOgvw5Kc",
        "outputId": "88ad0506-7a38-46ae-912a-78c915bda442"
      },
      "source": [
        "max_seq_length = 384\r\n",
        "bert_layer = hub.KerasLayer(\r\n",
        "    \"https://tfhub.dev/tensorflow/bert_en_uncased_L-12_H-768_A-12/3\",\r\n",
        "    trainable=True)\r\n",
        "!wget https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased-vocab.txt\r\n",
        "tokenizer = BertWordPieceTokenizer('bert-base-uncased-vocab.txt')"
      ],
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "error",
          "ename": "InvalidArgumentError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mInvalidArgumentError\u001b[0m                      Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-29-df2c4b0dbd14>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m bert_layer = hub.KerasLayer(\n\u001b[1;32m      3\u001b[0m     \u001b[0;34m\"https://tfhub.dev/tensorflow/bert_en_uncased_L-12_H-768_A-12/3\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m     trainable=True)# if train online\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0mget_ipython\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msystem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'wget https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased-vocab.txt'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0mtokenizer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mBertWordPieceTokenizer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'bert-base-uncased-vocab.txt'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow_hub/keras_layer.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, handle, trainable, arguments, _sentinel, tags, signature, signature_outputs_as_dict, output_key, output_shape, load_options, **kwargs)\u001b[0m\n\u001b[1;32m    150\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    151\u001b[0m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_load_options\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mload_options\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 152\u001b[0;31m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_func\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mload_module\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtags\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_load_options\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    153\u001b[0m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_has_training_argument\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc_has_training_argument\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_func\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    154\u001b[0m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_is_hub_module_v1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_func\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"_is_hub_module_v1\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow_hub/keras_layer.py\u001b[0m in \u001b[0;36mload_module\u001b[0;34m(handle, tags, load_options)\u001b[0m\n\u001b[1;32m    425\u001b[0m       \u001b[0;32mexcept\u001b[0m \u001b[0mImportError\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# Expected before TF2.4.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    426\u001b[0m         \u001b[0mset_load_options\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mload_options\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 427\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mmodule_v2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtags\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtags\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptions\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mset_load_options\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    428\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    429\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow_hub/module_v2.py\u001b[0m in \u001b[0;36mload\u001b[0;34m(handle, tags, options)\u001b[0m\n\u001b[1;32m    104\u001b[0m         module_path, tags=tags, options=options)\n\u001b[1;32m    105\u001b[0m   \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 106\u001b[0;31m     \u001b[0mobj\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mv1\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msaved_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_v2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodule_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtags\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtags\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    107\u001b[0m   \u001b[0mobj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_is_hub_module_v1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mis_hub_module_v1\u001b[0m  \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    108\u001b[0m   \u001b[0;32mreturn\u001b[0m \u001b[0mobj\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/saved_model/load.py\u001b[0m in \u001b[0;36mload\u001b[0;34m(export_dir, tags, options)\u001b[0m\n\u001b[1;32m    857\u001b[0m     \u001b[0mValueError\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mIf\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0mtags\u001b[0m\u001b[0;31m`\u001b[0m \u001b[0mdon\u001b[0m\u001b[0;31m'\u001b[0m\u001b[0mt\u001b[0m \u001b[0mmatch\u001b[0m \u001b[0ma\u001b[0m \u001b[0mMetaGraph\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mSavedModel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    858\u001b[0m   \"\"\"\n\u001b[0;32m--> 859\u001b[0;31m   \u001b[0;32mreturn\u001b[0m \u001b[0mload_internal\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mexport_dir\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtags\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"root\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    860\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    861\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/saved_model/load.py\u001b[0m in \u001b[0;36mload_internal\u001b[0;34m(export_dir, tags, options, loader_cls, filters)\u001b[0m\n\u001b[1;32m    888\u001b[0m       \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    889\u001b[0m         loader = loader_cls(object_graph_proto, saved_model_proto, export_dir,\n\u001b[0;32m--> 890\u001b[0;31m                             ckpt_options, filters)\n\u001b[0m\u001b[1;32m    891\u001b[0m       \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mNotFoundError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0merr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    892\u001b[0m         raise FileNotFoundError(\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/saved_model/load.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, object_graph_proto, saved_model_proto, export_dir, ckpt_options, filters)\u001b[0m\n\u001b[1;32m    159\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    160\u001b[0m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_load_all\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 161\u001b[0;31m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_restore_checkpoint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    162\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    163\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mnode\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_nodes\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/saved_model/load.py\u001b[0m in \u001b[0;36m_restore_checkpoint\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    485\u001b[0m                                   self._checkpoint_options).expect_partial()\n\u001b[1;32m    486\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 487\u001b[0;31m       \u001b[0mload_status\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msaver\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrestore\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvariables_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_checkpoint_options\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    488\u001b[0m     \u001b[0mload_status\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0massert_existing_objects_matched\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    489\u001b[0m     \u001b[0mcheckpoint\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mload_status\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_checkpoint\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/training/tracking/util.py\u001b[0m in \u001b[0;36mrestore\u001b[0;34m(self, save_path, options)\u001b[0m\n\u001b[1;32m   1335\u001b[0m         options=options)\n\u001b[1;32m   1336\u001b[0m     base.CheckpointPosition(\n\u001b[0;32m-> 1337\u001b[0;31m         checkpoint=checkpoint, proto_id=0).restore(self._graph_view.root)\n\u001b[0m\u001b[1;32m   1338\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1339\u001b[0m     \u001b[0;31m# Attached dependencies are not attached to the root, so should be restored\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/training/tracking/base.py\u001b[0m in \u001b[0;36mrestore\u001b[0;34m(self, trackable)\u001b[0m\n\u001b[1;32m    251\u001b[0m         \u001b[0;31m# This object's correspondence with a checkpointed object is new, so\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    252\u001b[0m         \u001b[0;31m# process deferred restorations for it and its dependencies.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 253\u001b[0;31m         \u001b[0mrestore_ops\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrackable\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_restore_from_checkpoint_position\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    254\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mrestore_ops\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    255\u001b[0m           \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_checkpoint\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnew_restore_ops\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrestore_ops\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/training/tracking/base.py\u001b[0m in \u001b[0;36m_restore_from_checkpoint_position\u001b[0;34m(self, checkpoint_position)\u001b[0m\n\u001b[1;32m    971\u001b[0m     restore_ops.extend(\n\u001b[1;32m    972\u001b[0m         current_position.checkpoint.restore_saveables(\n\u001b[0;32m--> 973\u001b[0;31m             tensor_saveables, python_saveables))\n\u001b[0m\u001b[1;32m    974\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mrestore_ops\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    975\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/training/tracking/util.py\u001b[0m in \u001b[0;36mrestore_saveables\u001b[0;34m(self, tensor_saveables, python_saveables)\u001b[0m\n\u001b[1;32m    306\u001b[0m              \"expecting %s\") % (tensor_saveables.keys(), validated_names))\n\u001b[1;32m    307\u001b[0m       new_restore_ops = functional_saver.MultiDeviceSaver(\n\u001b[0;32m--> 308\u001b[0;31m           validated_saveables).restore(self.save_path_tensor, self.options)\n\u001b[0m\u001b[1;32m    309\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mcontext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexecuting_eagerly\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    310\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrestore_op\u001b[0m \u001b[0;32min\u001b[0m \u001b[0msorted\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnew_restore_ops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/training/saving/functional_saver.py\u001b[0m in \u001b[0;36mrestore\u001b[0;34m(self, file_prefix, options)\u001b[0m\n\u001b[1;32m    343\u001b[0m       \u001b[0mrestore_ops\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_function_restore\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    344\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 345\u001b[0;31m       \u001b[0mrestore_ops\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrestore_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    346\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    347\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mcallback\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_after_restore_callbacks\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/training/saving/functional_saver.py\u001b[0m in \u001b[0;36mrestore_fn\u001b[0;34m()\u001b[0m\n\u001b[1;32m    319\u001b[0m       \u001b[0;32mfor\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msaver\u001b[0m \u001b[0;32min\u001b[0m \u001b[0msorted\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_single_device_savers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    320\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 321\u001b[0;31m           \u001b[0mrestore_ops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msaver\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrestore\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfile_prefix\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    322\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    323\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mrestore_ops\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/training/saving/functional_saver.py\u001b[0m in \u001b[0;36mrestore\u001b[0;34m(self, file_prefix, options)\u001b[0m\n\u001b[1;32m    114\u001b[0m                                           structured_restored_tensors):\n\u001b[1;32m    115\u001b[0m       restore_ops[saveable.name] = saveable.restore(\n\u001b[0;32m--> 116\u001b[0;31m           restored_tensors, restored_shapes=None)\n\u001b[0m\u001b[1;32m    117\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mrestore_ops\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    118\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/training/saving/saveable_object_util.py\u001b[0m in \u001b[0;36mrestore\u001b[0;34m(self, restored_tensors, restored_shapes)\u001b[0m\n\u001b[1;32m    128\u001b[0m     \u001b[0;31m# Copy the restored tensor to the variable's device.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    129\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_var_device\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 130\u001b[0;31m       \u001b[0mrestored_tensor\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0marray_ops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0midentity\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrestored_tensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    131\u001b[0m       return resource_variable_ops.shape_safe_assign_variable_handle(\n\u001b[1;32m    132\u001b[0m           self.handle_op, self._var_shape, restored_tensor)\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/util/dispatch.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    199\u001b[0m     \u001b[0;34m\"\"\"Call target, and fall back on dispatchers if there is a TypeError.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    200\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 201\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    202\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mTypeError\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    203\u001b[0m       \u001b[0;31m# Note: convert_to_eager_tensor currently raises a ValueError, not a\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/array_ops.py\u001b[0m in \u001b[0;36midentity\u001b[0;34m(input, name)\u001b[0m\n\u001b[1;32m    285\u001b[0m     \u001b[0;31m# variables. Variables have correct handle data when graph building.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    286\u001b[0m     \u001b[0minput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconvert_to_tensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 287\u001b[0;31m   \u001b[0mret\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgen_array_ops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0midentity\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    288\u001b[0m   \u001b[0;31m# Propagate handle data for happier shape inference for resource variables.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    289\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"_handle_data\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/gen_array_ops.py\u001b[0m in \u001b[0;36midentity\u001b[0;34m(input, name)\u001b[0m\n\u001b[1;32m   3931\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0m_result\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3932\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0m_core\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3933\u001b[0;31m       \u001b[0m_ops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mraise_from_not_ok_status\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3934\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0m_core\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_FallbackException\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3935\u001b[0m       \u001b[0;32mpass\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/ops.py\u001b[0m in \u001b[0;36mraise_from_not_ok_status\u001b[0;34m(e, name)\u001b[0m\n\u001b[1;32m   6860\u001b[0m   \u001b[0mmessage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessage\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m\" name: \"\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;34m\"\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   6861\u001b[0m   \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 6862\u001b[0;31m   \u001b[0msix\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mraise_from\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_status_to_exception\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   6863\u001b[0m   \u001b[0;31m# pylint: enable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   6864\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/six.py\u001b[0m in \u001b[0;36mraise_from\u001b[0;34m(value, from_value)\u001b[0m\n",
            "\u001b[0;31mInvalidArgumentError\u001b[0m: Unsuccessful TensorSliceReader constructor: Failed to get matching files on /tmp/tfhub_modules/590d7d0ea1d4e227b197a3512d641a1af6b36db1/variables/variables: Unimplemented: File system scheme '[local]' not implemented (file: '/tmp/tfhub_modules/590d7d0ea1d4e227b197a3512d641a1af6b36db1/variables/variables') [Op:Identity]"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ei7CLNtyFq06",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 387
        },
        "outputId": "adf8e7e0-7f2d-439e-8429-0a38be676337"
      },
      "source": [
        "train_squad_examples = create_squad_examples(raw_train_data)\r\n",
        "x_train, y_train = create_inputs_targets(train_squad_examples)\r\n",
        "print(f\"{len(train_squad_examples)} training points created.\")\r\n",
        "eval_squad_examples = create_squad_examples(raw_eval_data)\r\n",
        "x_eval, y_eval = create_inputs_targets(eval_squad_examples)\r\n",
        "print(f\"{len(eval_squad_examples)} evaluation points created.\")\r\n",
        "\r\n",
        "# tensorflow pipeline\r\n",
        "batch_size = 64\r\n",
        "#train = tf.data.Dataset.from_tensor_slices((x_train, y_train))\r\n",
        "#validation = tf.data.Dataset.from_tensor_slices((x_eval, y_eval))\r\n",
        "#train_dataset = train.cache()\r\n",
        "#train_dataset = train_dataset.shuffle(2048).repeat().batch(batch_size, drop_remainder=True)\r\n",
        "#train_dataset = train_dataset.prefetch(buffer_size=tf.data.experimental.AUTOTUNE)\r\n",
        "#val_dataset = validation.repeat().batch(batch_size, drop_remainder=True)"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-14-e0f8dba1b9ec>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtrain_squad_examples\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcreate_squad_examples\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mraw_train_data\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mx_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcreate_inputs_targets\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_squad_examples\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"{len(train_squad_examples)} training points created.\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0meval_squad_examples\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcreate_squad_examples\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mraw_eval_data\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mx_eval\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_eval\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcreate_inputs_targets\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0meval_squad_examples\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-7-d8050caf7dd7>\u001b[0m in \u001b[0;36mcreate_squad_examples\u001b[0;34m(raw_data)\u001b[0m\n\u001b[1;32m     14\u001b[0m                 \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m                     \u001b[0msquad_eg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mSample\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mquestion\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcontext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 16\u001b[0;31m                 \u001b[0msquad_eg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpreprocess\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     17\u001b[0m                 \u001b[0msquad_examples\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msquad_eg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0msquad_examples\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-6-f008de33c1ce>\u001b[0m in \u001b[0;36mpreprocess\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     15\u001b[0m     \u001b[0mquestion\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\" \"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mquestion\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m     \u001b[0;31m# tokenize context and question\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 17\u001b[0;31m     \u001b[0mtokenized_context\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtokenizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcontext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     18\u001b[0m     \u001b[0mtokenized_question\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtokenizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mquestion\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m     \u001b[0;31m# preprocess answer if this is a validation or training sample\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'tokenizer' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Sc0bKp-OGT_3"
      },
      "source": [
        "# Sanity check if all sample in evaluation have all_answers not none\r\n",
        "lst = [i for i in range(len(eval_squad_examples)) if eval_squad_examples[i].all_answers == None and eval_squad_examples[i].skip == False]\r\n",
        "print(f\"There are {len(lst)} samples have no answers.\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "B0IzWRmVtou4"
      },
      "source": [
        "len([i for i in range(len(y_eval[0])) if y_eval[1][i] == -1 and y_eval[0][i] == -1])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LIv78pkmWiDU"
      },
      "source": [
        "# create model\r\n",
        "def create_model():\r\n",
        "    bert_layer = hub.KerasLayer(\"https://tfhub.dev/tensorflow/bert_en_uncased_L-12_H-768_A-12/2\", trainable=True)\r\n",
        "\r\n",
        "    input_word_ids = tf.keras.layers.Input(shape=(max_seq_length,), dtype=tf.int32, name='input_word_ids')\r\n",
        "    input_mask = tf.keras.layers.Input(shape=(max_seq_length,), dtype=tf.int32, name='input_mask')\r\n",
        "    input_type_ids = tf.keras.layers.Input(shape=(max_seq_length,), dtype=tf.int32, name='input_type_ids')\r\n",
        "    pooled_output, sequence_output = bert_layer([input_word_ids, input_mask, input_type_ids])\r\n",
        "    start_logits = tf.keras.layers.Dense(1, name=\"start_logit\", use_bias=False)(sequence_output)\r\n",
        "    start_logits = tf.keras.layers.Flatten()(start_logits)\r\n",
        "    end_logits = tf.keras.layers.Dense(1, name=\"end_logit\", use_bias=False)(sequence_output)\r\n",
        "    end_logits = tf.keras.layers.Flatten()(end_logits)\r\n",
        "    start_probs = tf.keras.layers.Activation(tf.keras.activations.softmax)(start_logits)\r\n",
        "    end_probs = tf.keras.layers.Activation(tf.keras.activations.softmax)(end_logits)\r\n",
        "    model = keras.Model(inputs=[input_word_ids, input_mask, input_type_ids], outputs=[start_probs, end_probs])\r\n",
        "    \r\n",
        "    return model"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tNT07rSVxJhY"
      },
      "source": [
        "class CustomizedLoss(tf.keras.losses.Loss):\r\n",
        "    def compute_loss(self, log_probs, positions):\r\n",
        "        one_hot_positions = tf.one_hot(\r\n",
        "            positions, depth=max_seq_length, dtype=tf.float32)\r\n",
        "        loss = -tf.reduce_mean(\r\n",
        "            tf.reduce_sum(one_hot_positions * log_probs, axis=-1))\r\n",
        "        return loss\r\n",
        "    \r\n",
        "    def call(self, y_true, y_pred):\r\n",
        "        start_loss = self.compute_loss(y_pred[0], y_true[0])\r\n",
        "        end_loss = self.compute_loss(y_pred[1], y_true[1])\r\n",
        "        #total_loss = (start_loss + end_loss) / 2.0\r\n",
        "        return start_loss, end_loss"
      ],
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0txvciDfT3xi",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 238
        },
        "outputId": "109ec6c4-44a4-4e1b-ae6d-ac139e5cb437"
      },
      "source": [
        "with tpu_strategy.scope(): # creating the model in the TPUStrategy scope means we will train the model on the TPU\r\n",
        "    model = create_model()\r\n",
        "    # setup compile\r\n",
        "    #loss = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=False)\r\n",
        "    optimizer = tf.keras.optimizers.Adam(lr=1e-5, beta_1=0.9, beta_2=0.98, epsilon=1e-9)\r\n",
        "    model.compile(optimizer=optimizer, loss=CustomizedLoss)\r\n",
        "    model.fit(x_train, y_train, epochs=1, batch_size=batch_size)"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "error",
          "ename": "TypeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-20-85ae04829848>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0;31m#loss = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=False)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0moptimizer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptimizers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mAdam\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlr\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1e-5\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbeta_1\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.9\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbeta_2\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.98\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepsilon\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1e-9\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m     \u001b[0mstart_loss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mend_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mCustomizedLoss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m     \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstart_loss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mend_loss\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m     \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mTypeError\u001b[0m: 'CustomizedLoss' object is not iterable"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "v8TAHVeC8ba_"
      },
      "source": [
        "model.summary()\r\n",
        "tf.keras.utils.plot_model(model, \"./multi_input_and_output_model.png\", show_shapes=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dNP2K8Na4wxN"
      },
      "source": [
        "##Training"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "svbAXn-xbtrh"
      },
      "source": [
        "model.fit(x_train, y_train, epochs=1, batch_size=batch_size)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "D_R-yJwU31qk"
      },
      "source": [
        "model.save_weights(\"./weights.h5\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H2cYcF0_Vvxv"
      },
      "source": [
        "#4.Evaluate the QA model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1ih3s4fvBSDE"
      },
      "source": [
        "model.load_weights(\"./weights.h5\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "H5fDpl0Cpf-3"
      },
      "source": [
        "def normalize_text(text):\r\n",
        "    # convert to lower case\r\n",
        "    text = text.lower()\r\n",
        "    # remove redundant whitespaces\r\n",
        "    text = \"\".join(ch for ch in text if ch not in set(string.punctuation))\r\n",
        "    # remove articles\r\n",
        "    regex = re.compile(r\"\\b(a|an|the)\\b\", re.UNICODE)\r\n",
        "    text = re.sub(regex, \" \", text)\r\n",
        "    text = \" \".join(text.split())\r\n",
        "    return text"
      ],
      "execution_count": 40,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "edTQL90wo1CA"
      },
      "source": [
        "##F1 score"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0_YRaNx2owYT"
      },
      "source": [
        "def compute_f1(raw_test_data):\r\n",
        "    test_squad_samples = create_squad_examples(raw_test_data)\r\n",
        "    x_test, _ = create_inputs_targets(test_squad_samples)\r\n",
        "    pred_start, pred_end = model.predict(x_test)\r\n",
        "\r\n",
        "    test_examples_no_skip = [_ for _ in test_squad_samples if _.skip == False]\r\n",
        "    \r\n",
        "    f1_scores = []\r\n",
        "    for idx, (start, end) in enumerate(zip(pred_start, pred_end)):\r\n",
        "        # take the required Sample object with the ground-truth answers in it\r\n",
        "        squad_eg = test_examples_no_skip[idx]\r\n",
        "        # use offsets to get back the span of text corresponding to\r\n",
        "        # our predicted first and last tokens\r\n",
        "        offsets = squad_eg.context_token_to_char\r\n",
        "        start = np.argmax(start)\r\n",
        "        end = np.argmax(end)\r\n",
        "        # if start index is larger than the length of tokens\r\n",
        "        if start >= len(offsets):\r\n",
        "            f1_scores.append(0)\r\n",
        "            continue\r\n",
        "        \r\n",
        "        pred_char_start = offsets[start][0]\r\n",
        "        if end < len(offsets):\r\n",
        "            pred_char_end = offsets[end][1]\r\n",
        "            pred_ans = squad_eg.context[pred_char_start:pred_char_end]\r\n",
        "        else:\r\n",
        "            pred_ans = squad_eg.context[pred_char_start:]\r\n",
        "        \r\n",
        "        normalized_pred_ans = normalize_text(pred_ans)\r\n",
        "        pred_tokens = normalized_pred_ans.split()\r\n",
        "        \r\n",
        "        # if predicted answers is none, f1 score = 0 if all_answers is none, else 1\r\n",
        "        if not squad_eg.all_answers:\r\n",
        "            f1_scores.append(len(pred_tokens) == 0)\r\n",
        "            continue\r\n",
        "        if len(pred_tokens) == 0:\r\n",
        "            f1_scores.append(squad_eg.all_answers == None)\r\n",
        "            continue\r\n",
        "\r\n",
        "        max_f1 = float('-inf')\r\n",
        "        normalized_true_ans_lst = [normalize_text(_) for _ in squad_eg.all_answers]\r\n",
        "        for normalized_true_ans in normalized_true_ans_lst:\r\n",
        "            truth_tokens = normalized_true_ans.split()\r\n",
        "            common_tokens = set(pred_tokens) & set(truth_tokens)\r\n",
        "            # if there are no common tokens then f1 = 0\r\n",
        "            if len(common_tokens) == 0:\r\n",
        "                max_f1 = max(max_f1, 0)\r\n",
        "            \r\n",
        "            else:\r\n",
        "                prec = len(common_tokens) / len(pred_tokens)\r\n",
        "                rec = len(common_tokens) / len(truth_tokens)   \r\n",
        "                max_f1 = max(max_f1, 2 * (prec * rec) / (prec + rec))\r\n",
        "        \r\n",
        "        f1_scores.append(max_f1)\r\n",
        "\r\n",
        "    print(f\"f1 score={np.mean(f1_scores):.2f}\")\r\n",
        "    return f1_scores"
      ],
      "execution_count": 45,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SYweu-fsPqOc",
        "outputId": "808a5958-83f1-4b4d-80e4-fc8702dd0f4a"
      },
      "source": [
        "f1_scores = compute_f1(raw_eval_data)"
      ],
      "execution_count": 46,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "f1 score=0.66\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X2dwt1cNo3F8"
      },
      "source": [
        "##EM (exact match)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QiRyM9qgwjlP"
      },
      "source": [
        "def exact_match_score(raw_test_data):\r\n",
        "    test_squad_samples = create_squad_examples(raw_test_data)\r\n",
        "    x_test, _ = create_inputs_targets(test_squad_samples)\r\n",
        "    pred_start, pred_end = model.predict(x_test)\r\n",
        "\r\n",
        "    test_examples_no_skip = [_ for _ in test_squad_samples if _.skip == False]\r\n",
        "    \r\n",
        "    count = 0\r\n",
        "\r\n",
        "    for idx, (start, end) in enumerate(zip(pred_start, pred_end)):\r\n",
        "        # take the required Sample object with the ground-truth answers in it\r\n",
        "        squad_eg = test_examples_no_skip[idx]\r\n",
        "        # use offsets to get back the span of text corresponding to\r\n",
        "        # our predicted first and last tokens\r\n",
        "        offsets = squad_eg.context_token_to_char\r\n",
        "        start = np.argmax(start)\r\n",
        "        end = np.argmax(end)\r\n",
        "        if start >= len(offsets):\r\n",
        "            continue\r\n",
        "        pred_char_start = offsets[start][0]\r\n",
        "        if end < len(offsets):\r\n",
        "            pred_char_end = offsets[end][1]\r\n",
        "            pred_ans = squad_eg.context[pred_char_start:pred_char_end]\r\n",
        "        else:\r\n",
        "            pred_ans = squad_eg.context[pred_char_start:]\r\n",
        "        normalized_pred_ans = normalize_text(pred_ans)\r\n",
        "\r\n",
        "        # special case for all_answers is None\r\n",
        "        if not squad_eg.all_answers:\r\n",
        "            if len(pred_ans) == 0:\r\n",
        "                count += 1\r\n",
        "            else:\r\n",
        "                continue \r\n",
        "        \r\n",
        "        # clean the real answers\r\n",
        "        normalized_true_ans_lst = [normalize_text(_) for _ in squad_eg.all_answers]\r\n",
        "        # check if the predicted answer is in an array of the ground-truth answers\r\n",
        "        if normalized_pred_ans in normalized_true_ans_lst:\r\n",
        "            count += 1\r\n",
        "    acc = count / len(x_test[0])\r\n",
        "    print(f\"exact match score={acc:.2f}\")"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 167
        },
        "id": "_OzfPRb1Hh5t",
        "outputId": "8d3699eb-4f9a-44cc-b247-95a88d90d201"
      },
      "source": [
        "exact_match_score(raw_eval_data)"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-2-b576a5a7501e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mexact_match_score\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mraw_eval_data\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m: name 'raw_eval_data' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a9Z-DNTHox3-"
      },
      "source": [
        "##Examples"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Z3f9OMcUll8o"
      },
      "source": [
        "def evaluate_qa_model(raw_test_data, verbose=True):\r\n",
        "    test_samples = create_squad_examples(raw_test_data)\r\n",
        "    x_test, _ = create_inputs_targets(test_samples)\r\n",
        "    pred_start, pred_end = model.predict(x_test)\r\n",
        "    pred_ans_lst = []\r\n",
        "    for idx, (start, end) in enumerate(zip(pred_start, pred_end)):\r\n",
        "        test_sample = test_samples[idx]\r\n",
        "        offsets = test_sample.context_token_to_char\r\n",
        "        start = np.argmax(start)\r\n",
        "        end = np.argmax(end)\r\n",
        "        pred_ans = None\r\n",
        "        if start >= len(offsets):\r\n",
        "            continue\r\n",
        "        pred_char_start = offsets[start][0]\r\n",
        "        if end < len(offsets):\r\n",
        "            pred_char_end = offsets[end][1]\r\n",
        "            pred_ans = test_sample.context[pred_char_start:pred_char_end]\r\n",
        "        else:\r\n",
        "            pred_ans = test_sample.context[pred_char_start:]\r\n",
        "        \r\n",
        "        pred_ans_lst.append(pred_ans)\r\n",
        "\r\n",
        "        if verbose: \r\n",
        "            print(\"Q: \" + test_sample.question)\r\n",
        "            print(\"A: \" + pred_ans)\r\n",
        "    \r\n",
        "    return pred_ans_lst"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fuuiC3rOmhwI",
        "outputId": "c1b589ab-c2f4-4e75-b135-0ba09e573607"
      },
      "source": [
        "data = {\"data\":\r\n",
        "    [\r\n",
        "        {\"title\": \"Clinical note\",\r\n",
        "         \"paragraphs\": [\r\n",
        "             {\r\n",
        "                 \"context\": \"Abnormal echocardiogram findings and followup. Shortness of breath, congestive heart failure, \"\r\n",
        "                            \"and valvular insufficiency. The patient complains of shortness of breath, which is worsening. \"\r\n",
        "                            \"The patient underwent an echocardiogram, which shows severe mitral regurgitation and also large \"\r\n",
        "                            \"pleural effusion. The patient is an 86-year-old female admitted for evaluation of abdominal pain \"\r\n",
        "                            \"and bloody stools. The patient has colitis and also diverticulitis, undergoing treatment.\"\r\n",
        "                            \"During the hospitalization, the patient complains of shortness of breath, which is worsening.\"\r\n",
        "                            \"The patient underwent an echocardiogram, which shows severe mitral regurgitation and also large \"\r\n",
        "                            \"pleural effusion. This consultation is for further evaluation in this regard. As per the patient, \"\r\n",
        "                            \"she is an 86-year-old female, has limited activity level. She has been having shortness of breath \"\r\n",
        "                            \"for many years. She also was told that she has a heart murmur, which was not followed through \"\r\n",
        "                            \"on a regular basis.\",\r\n",
        "                 \"qas\": [\r\n",
        "                     {\"question\": \"How old is the patient?\",\r\n",
        "                      \"id\": \"Q1\"\r\n",
        "                      },\r\n",
        "                     {\"question\": \"Does the patient have any complaints?\",\r\n",
        "                      \"id\": \"Q2\"\r\n",
        "                      },\r\n",
        "                     {\"question\":\"What is the reason for this consultation?\",\r\n",
        "                      \"id\": \"Q3\"\r\n",
        "                      },\r\n",
        "                     {\"question\": \"What does her echocardiogram show?\",\r\n",
        "                      \"id\": \"Q4\"\r\n",
        "                      },\r\n",
        "                     {\"question\": \"What other symptoms does the patient have?\",\r\n",
        "                      \"id\": \"Q5\"\r\n",
        "                      },\r\n",
        "                     {\"question\": \"What's the gender of this patient?\",\r\n",
        "                      \"id\": \"Q6\"\r\n",
        "                      }\r\n",
        "                 ]}]}]}\r\n",
        "\r\n",
        "pred_ans_lst = evaluate_qa_model(data)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Q: How old is the patient?\n",
            "A: 86\n",
            "Q: Does the patient have any complaints?\n",
            "A: The patient complains of shortness of breath\n",
            "Q: What is the reason for this consultation?\n",
            "A: for further evaluation\n",
            "Q: What does her echocardiogram show?\n",
            "A: severe mitral regurgitation and also large pleural effusion\n",
            "Q: What other symptoms does the patient have?\n",
            "A: colitis and also diverticulitis\n",
            "Q: What's the gender of this patient?\n",
            "A: 86-year-old female\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9fBHOr60j7Yj",
        "outputId": "4b71e846-2b99-42eb-c8e2-53f75859ba7f"
      },
      "source": [
        "data = {\"data\":\r\n",
        "    [\r\n",
        "        {\"title\": \"Project Apollo\",\r\n",
        "         \"paragraphs\": [\r\n",
        "             {\r\n",
        "                 \"context\": \"The Apollo program, also known as Project Apollo, was the third United States human \"\r\n",
        "                            \"spaceflight program carried out by the National Aeronautics and Space Administration (\"\r\n",
        "                            \"NASA), which accomplished landing the first humans on the Moon from 1969 to 1972. First \"\r\n",
        "                            \"conceived during Dwight D. Eisenhower's administration as a three-man spacecraft to \"\r\n",
        "                            \"follow the one-man Project Mercury which put the first Americans in space, Apollo was \"\r\n",
        "                            \"later dedicated to President John F. Kennedy's national goal of landing a man on the \"\r\n",
        "                            \"Moon and returning him safely to the Earth by the end of the 1960s, which he proposed in \"\r\n",
        "                            \"a May 25, 1961, address to Congress. Project Mercury was followed by the two-man Project \"\r\n",
        "                            \"Gemini. The first manned flight of Apollo was in 1968. Apollo ran from 1961 to 1972, \"\r\n",
        "                            \"and was supported by the two man Gemini program which ran concurrently with it from 1962 \"\r\n",
        "                            \"to 1966. Gemini missions developed some of the space travel techniques that were \"\r\n",
        "                            \"necessary for the success of the Apollo missions. Apollo used Saturn family rockets as \"\r\n",
        "                            \"launch vehicles. Apollo/Saturn vehicles were also used for an Apollo Applications \"\r\n",
        "                            \"Program, which consisted of Skylab, a space station that supported three manned missions \"\r\n",
        "                            \"in 1973-74, and the Apollo-Soyuz Test Project, a joint Earth orbit mission with the \"\r\n",
        "                            \"Soviet Union in 1975.\",\r\n",
        "                 \"qas\": [\r\n",
        "                     {\"question\": \"What project put the first Americans into space?\",\r\n",
        "                      \"id\": \"Q1\"\r\n",
        "                      },\r\n",
        "                     {\"question\": \"What program was created to carry out these projects and missions?\",\r\n",
        "                      \"id\": \"Q2\"\r\n",
        "                      },\r\n",
        "                     {\"question\": \"What year did the first manned Apollo flight occur?\",\r\n",
        "                      \"id\": \"Q3\"\r\n",
        "                      },\r\n",
        "                     {\"question\": \"What President is credited with the original notion of putting Americans in space?\",\r\n",
        "                      \"id\": \"Q4\"\r\n",
        "                      },\r\n",
        "                     {\"question\": \"Who did the U.S. collaborate with on an Earth orbit mission in 1975?\",\r\n",
        "                      \"id\": \"Q5\"\r\n",
        "                      },\r\n",
        "                     {\"question\": \"How long did Project Apollo run?\",\r\n",
        "                      \"id\": \"Q6\"\r\n",
        "                      },\r\n",
        "                     {\"question\": \"What program helped develop space travel techniques that Project Apollo used?\",\r\n",
        "                      \"id\": \"Q7\"\r\n",
        "                      },\r\n",
        "                     {\"question\": \"What space station supported three manned missions in 1973-1974?\",\r\n",
        "                      \"id\": \"Q8\"\r\n",
        "                      }\r\n",
        "                 ]}]}]}\r\n",
        "\r\n",
        "\r\n",
        "pred_ans_lst = evaluate_qa_model(data)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Q: What project put the first Americans into space?\n",
            "A: Project Mercury\n",
            "Q: What program was created to carry out these projects and missions?\n",
            "A: Project Apollo\n",
            "Q: What year did the first manned Apollo flight occur?\n",
            "A: 1968\n",
            "Q: What President is credited with the original notion of putting Americans in space?\n",
            "A: John F. Kennedy\n",
            "Q: Who did the U.S. collaborate with on an Earth orbit mission in 1975?\n",
            "A: Soviet Union\n",
            "Q: How long did Project Apollo run?\n",
            "A: 1961 to 1972\n",
            "Q: What program helped develop space travel techniques that Project Apollo used?\n",
            "A: Gemini\n",
            "Q: What space station supported three manned missions in 1973-1974?\n",
            "A: Skylab\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZtcLf9j0Zlf3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "429c8216-38b8-4236-daf4-b3d2b3a11c5b"
      },
      "source": [
        "data = {\"data\":\r\n",
        "    [\r\n",
        "        {\"title\": \"Project Apollo\",\r\n",
        "         \"paragraphs\": [\r\n",
        "             {\r\n",
        "                 \"context\": \"The word pharmacy is derived from its root word pharma which was a term used since \"\r\n",
        "                            \"the 15thâ€“17th centuries. However, the original Greek roots from pharmakos imply sorcery \"\r\n",
        "                            \"or even poison. In addition to pharma responsibilities, the pharma offered general medical \"\r\n",
        "                            \"advice and a range of services that are now performed solely by other specialist practitioners, \"\r\n",
        "                            \"such as surgery and midwifery. The pharma (as it was referred to) often operated through a \"\r\n",
        "                            \"retail shop which, in addition to ingredients for medicines, sold tobacco and patent medicines. \"\r\n",
        "                            \"Often the place that did this was called an apothecary and several languages have this as the \"\r\n",
        "                            \"dominant term, though their practices are more akin to a modern pharmacy, in English the term \"\r\n",
        "                            \"apothecary would today be seen as outdated or only approproriate if herbal remedies were on offer \"\r\n",
        "                            \"to a large extent. The pharmas also used many other herbs not listed. The Greek word Pharmakeia \"\r\n",
        "                            \"(Greek: Ï†Î±ÏÎ¼Î±ÎºÎµÎ¯Î±) derives from pharmakon (Ï†Î¬ÏÎ¼Î±ÎºÎ¿Î½), meaning 'drug', 'medicine' (or 'poison').\",\r\n",
        "                 \"qas\": [\r\n",
        "                     {\"question\": \"What word is the word pharmacy taken from??\",\r\n",
        "                      \"id\": \"Q1\"\r\n",
        "                      },\r\n",
        "                    {\"question\": \"When is the word pharmacy derived??\",\r\n",
        "                      \"id\": \"Q2\"\r\n",
        "                      },\r\n",
        "                    {\"question\": \"How did pharma operate??\",\r\n",
        "                      \"id\": \"Q3\"\r\n",
        "                      }\r\n",
        "                 ]}]}]}\r\n",
        "\r\n",
        "\r\n",
        "pred_ans_lst = evaluate_qa_model(data)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Q: What word is the word pharmacy taken from??\n",
            "A: pharma\n",
            "Q: When is the word pharmacy derived??\n",
            "A: 15thâ€“17th centuries\n",
            "Q: How did pharma operate??\n",
            "A: through a retail shop\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CkHR8UBXVxyv"
      },
      "source": [
        "#5.Deploy the model on Django"
      ]
    }
  ]
}