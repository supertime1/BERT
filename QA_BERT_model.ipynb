{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "QA BERT model.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyP+aXFktPr6QAnKax+EbyrM",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/supertime1/BERT/blob/main/QA_BERT_model.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xz4W_-pMVJsX"
      },
      "source": [
        "#1.Import Dependencies"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "orAMKxKkU_Vh"
      },
      "source": [
        "import os\r\n",
        "import tensorflow as tf\r\n",
        "import tensorflow_hub as hub\r\n",
        "import tensorflow_datasets as tfds\r\n",
        "import tensorflow_text as text  # A dependency of the preprocessing model\r\n",
        "import tensorflow_addons as tfa\r\n",
        "from official.nlp import optimization\r\n",
        "import numpy as np\r\n",
        "import json\r\n",
        "\r\n",
        "tf.get_logger().setLevel('ERROR')"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8xr0VeYgYyKp"
      },
      "source": [
        "import sys\r\n",
        "sys.path.append(r'C:\\Users\\57lzhang.US04WW4008\\Documents\\GitHub\\BERT')"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Zlt53u2UVlb-"
      },
      "source": [
        "#2.Preprocess the input data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-3Jk1xA2YZf2"
      },
      "source": [
        "dataset_path = r'C:\\Users\\57lzhang.US04WW4008\\Documents\\GitHub\\BERT\\dataset'\r\n",
        "train_ds = os.path.join(dataset_path, 'train-v2.0.json')\r\n",
        "with open(train_ds) as f:\r\n",
        "  data = json.load(f)"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4Na_RFxxWIdS"
      },
      "source": [
        "def extract_data_label_pairs(dataset_path):\r\n",
        "  \r\n",
        "  pairs = []\r\n",
        "  labels = []\r\n",
        "  \r\n",
        "  #load training data\r\n",
        "  train_ds = os.path.join(dataset_path, 'train-v2.0.json')\r\n",
        "  with open(train_ds) as f:\r\n",
        "    data = json.load(f)\r\n",
        "\r\n",
        "  #extract passage\r\n",
        "  for topic in data['data']:\r\n",
        "    for segment in topic['paragraphs']:\r\n",
        "        passage = segment['context']\r\n",
        "        questions = segment['qas']\r\n",
        "        for q_dict in questions:\r\n",
        "          question = q_dict['question']\r\n",
        "          \r\n",
        "          #if answer exists in the context\r\n",
        "          if not q_dict['is_impossible']:\r\n",
        "            #extract labels i.e. answer start and end indices\r\n",
        "            answers = q_dict['answers']\r\n",
        "            answer_start_index = answers[0]['answer_start']\r\n",
        "            answer_end_index = len(answers[0]['text']) + answer_start_index - 1\r\n",
        "            pairs.append([question, passage])\r\n",
        "            labels.append([answer_start_index, answer_end_index])\r\n",
        "\r\n",
        "          #if answer does not exist in the context\r\n",
        "          else:\r\n",
        "            pairs.append([question, passage])\r\n",
        "            labels.append([-1, -1])\r\n",
        "\r\n",
        "  return pairs, labels"
      ],
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rowRywB5cj95",
        "outputId": "f3588216-c399-49e0-8a23-691cd6a7d786"
      },
      "source": [
        "pairs, labels = extract_data_label_pairs(r'C:\\Users\\57lzhang.US04WW4008\\Documents\\GitHub\\BERT\\dataset')\r\n",
        "print(f'There are {sum([1 for i in range(len(pairs)) if labels[i] != [-1, -1]])} Question-Answer pairs,')\r\n",
        "print(f'and {sum([1 for i in range(len(pairs)) if labels[i] == [-1, -1]])} Questions that have no answers')"
      ],
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "There are 86821 Question-Answer pairs,\n",
            "and 43498 Questions that have no answers\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IWmFQmyxgeD4"
      },
      "source": [
        "tfhub_handle_encoder = 'https://tfhub.dev/tensorflow/albert_en_base/2'\r\n",
        "tfhub_handle_preprocess = \"https://tfhub.dev/tensorflow/albert_en_preprocess/2\""
      ],
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "339SaBq6b9Rq"
      },
      "source": [
        "def make_bert_preprocess_model(sentence_features, seq_length=512):\r\n",
        "  \"\"\"Returns Model mapping string features to BERT inputs.\r\n",
        "\r\n",
        "  Args:\r\n",
        "    sentence_features: a list with the names of string-valued features.\r\n",
        "    seq_length: an integer that defines the sequence length of BERT inputs.\r\n",
        "\r\n",
        "  Returns:\r\n",
        "    A Keras Model that can be called on a list or dict of string Tensors\r\n",
        "    (with the order or names, resp., given by sentence_features) and\r\n",
        "    returns a dict of tensors for input to BERT.\r\n",
        "  \"\"\"\r\n",
        "\r\n",
        "  input_segments = [\r\n",
        "      tf.keras.layers.Input(shape=(), dtype=tf.string, name=ft)\r\n",
        "      for ft in sentence_features]\r\n",
        "\r\n",
        "  # Tokenize the text to word pieces.\r\n",
        "  bert_preprocess = hub.load(tfhub_handle_preprocess)\r\n",
        "  tokenizer = hub.KerasLayer(bert_preprocess.tokenize, name='tokenizer')\r\n",
        "  segments = [tokenizer(s) for s in input_segments]\r\n",
        "\r\n",
        "  # Optional: Trim segments in a smart way to fit seq_length.\r\n",
        "  # Simple cases (like this example) can skip this step and let\r\n",
        "  # the next step apply a default truncation to approximately equal lengths.\r\n",
        "  truncated_segments = segments\r\n",
        "\r\n",
        "  # Pack inputs. The details (start/end token ids, dict of output tensors)\r\n",
        "  # are model-dependent, so this gets loaded from the SavedModel.\r\n",
        "  packer = hub.KerasLayer(bert_preprocess.bert_pack_inputs,\r\n",
        "                          arguments=dict(seq_length=seq_length),\r\n",
        "                          name='packer')\r\n",
        "  model_inputs = packer(truncated_segments)\r\n",
        "  return tf.keras.Model(input_segments, model_inputs)"
      ],
      "execution_count": 36,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "L2_20AL4gQIj",
        "outputId": "10641361-cfbf-4ee2-956b-c33fff6cf4ce"
      },
      "source": [
        "test_preprocess_model = make_bert_preprocess_model(['my_input1', 'my_input2'])\r\n",
        "test_text = [np.array([pairs[0][0]]),\r\n",
        "             np.array([pairs[0][1]])]\r\n",
        "\r\n",
        "text_preprocessed = test_preprocess_model(test_text)\r\n",
        "\r\n",
        "print('Question       : ', pairs[0][0])\r\n",
        "print('Context        : ', pairs[0][1])\r\n",
        "print('Keys           : ', list(text_preprocessed.keys()))\r\n",
        "print('Shape Word Ids : ', text_preprocessed['input_word_ids'].shape)\r\n",
        "print('Word Ids       : ', text_preprocessed['input_word_ids'][0, :50])\r\n",
        "print('Shape Mask     : ', text_preprocessed['input_mask'].shape)\r\n",
        "print('Input Mask     : ', text_preprocessed['input_mask'][0, :50])\r\n",
        "print('Shape Type Ids : ', text_preprocessed['input_type_ids'].shape)\r\n",
        "print('Type Ids       : ', text_preprocessed['input_type_ids'][0, :50])"
      ],
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Question       :  When did Beyonce start becoming popular?\n",
            "Context        :  Beyoncé Giselle Knowles-Carter (/biːˈjɒnseɪ/ bee-YON-say) (born September 4, 1981) is an American singer, songwriter, record producer and actress. Born and raised in Houston, Texas, she performed in various singing and dancing competitions as a child, and rose to fame in the late 1990s as lead singer of R&B girl-group Destiny's Child. Managed by her father, Mathew Knowles, the group became one of the world's best-selling girl groups of all time. Their hiatus saw the release of Beyoncé's debut album, Dangerously in Love (2003), which established her as a solo artist worldwide, earned five Grammy Awards and featured the Billboard Hot 100 number-one singles \"Crazy in Love\" and \"Baby Boy\".\n",
            "Keys           :  ['input_type_ids', 'input_word_ids', 'input_mask']\n",
            "Shape Word Ids :  (1, 512)\n",
            "Word Ids       :  tf.Tensor(\n",
            "[    2    76   144 24809   799  1535   844    60     3 24809 16004  3745\n",
            "   143  1355     8  1367   815    13     5   118  2161     1   728     1\n",
            " 23157     1   118 12092     8  7370     8  6366     6    13     5   381\n",
            "   299  2193  2229     6    25    40   189  1377    15  7815    15   571\n",
            "  1421    17], shape=(50,), dtype=int32)\n",
            "Shape Mask     :  (1, 512)\n",
            "Input Mask     :  tf.Tensor(\n",
            "[1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
            " 1 1 1 1 1 1 1 1 1 1 1 1 1], shape=(50,), dtype=int32)\n",
            "Shape Type Ids :  (1, 512)\n",
            "Type Ids       :  tf.Tensor(\n",
            "[0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
            " 1 1 1 1 1 1 1 1 1 1 1 1 1], shape=(50,), dtype=int32)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Y-XT_UKrndwN"
      },
      "source": [
        "train_input = [make_bert_preprocess_model(i) for i in pairs]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wqDGaP49iMoL"
      },
      "source": [
        "train_dataset = tf.data.Dataset.from_tensor_slices(pairs)\r\n",
        "train_lb = tf.data.Dataset.from_tensor_slices(labels)\r\n",
        "train = tf.data.Dataset.zip((train_ds, train_lb))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Uv4MbOSUfJa-"
      },
      "source": [
        "def prepare_training_input(pairs, tokenizer, max_seq_length=512):\r\n",
        "  lst = []\r\n",
        "  for question, passage, _ in pairs:\r\n",
        "    if len(passage) > max_seq_length:\r\n",
        "      continue\r\n",
        "\r\n",
        "    lst.append((prepare_bert_input(question, passage, tokenizer, max_seq_length=max_seq_length)))\r\n",
        "  \r\n",
        "  return lst"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UzOxVB62K6sw"
      },
      "source": [
        "AUTOTUNE = tf.data.experimental.AUTOTUNE\r\n",
        "train_ds = tf.data.Dataset.from_tensor_slices(pairs)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eGYiXACIPplO"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hpw2lCWCgspi"
      },
      "source": [
        "#find the tokenizer \r\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"bert-large-uncased-whole-word-masking-finetuned-squad\")"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hvX_Xm1AVotx"
      },
      "source": [
        "def prepare_bert_input(question, passage, tokenizer, max_seq_length=384):\r\n",
        "    \"\"\"\r\n",
        "    Prepare question and passage for input to BERT. \r\n",
        "\r\n",
        "    Args:\r\n",
        "        question (string): question string\r\n",
        "        passage (string): passage string where answer should lie\r\n",
        "        tokenizer (Tokenizer): used for transforming raw string input\r\n",
        "        max_seq_length (int): length of BERT input\r\n",
        "    o\r\n",
        "    Returns:\r\n",
        "        input_ids (tf.Tensor): tensor of size (1, max_seq_length) which holds\r\n",
        "                               ids of tokens in input\r\n",
        "        input_mask (list): list of length max_seq_length of 1s and 0s with 1s\r\n",
        "                           in indices corresponding to input tokens, 0s in\r\n",
        "                           indices corresponding to padding\r\n",
        "        tokens (list): list of length of actual string tokens corresponding to input_ids\r\n",
        "    \"\"\"\r\n",
        "    # tokenize question\r\n",
        "    question_tokens = tokenizer.tokenize(question)\r\n",
        "    \r\n",
        "    # tokenize passage\r\n",
        "    passage_token = tokenizer.tokenize(passage)\r\n",
        "\r\n",
        "    # get special tokens \r\n",
        "    CLS = tokenizer.cls_token\r\n",
        "    SEP = tokenizer.sep_token\r\n",
        "        \r\n",
        "    # manipulate tokens to get input in correct form (not adding padding yet)\r\n",
        "    # CLS {question_tokens} SEP {answer_tokens} \r\n",
        "    # This should be a list of tkens\r\n",
        "    tokens = [CLS] + question_tokens + [SEP] + passage_token\r\n",
        "\r\n",
        "    \r\n",
        "    # Convert tokens into integer IDs.\r\n",
        "    input_ids = tokenizer.convert_tokens_to_ids(tokens)\r\n",
        "    \r\n",
        "    # Create an input mask which has integer 1 for each token in the 'tokens' list\r\n",
        "    input_mask = [1]*len(tokens)\r\n",
        "\r\n",
        "    # pad input_ids with 0s until it is the max_seq_length\r\n",
        "    # Create padding for input_ids by creating a list of zeros [0,0,...0]\r\n",
        "    # Add the padding to input_ids so that its length equals max_seq_length\r\n",
        "    input_ids = input_ids + [0]*(max_seq_length - len(tokens))\r\n",
        "    \r\n",
        "    # Do the same to pad the input_mask so its length is max_seq_length\r\n",
        "    input_mask = input_mask + [0]*(max_seq_length - len(input_mask))\r\n",
        "\r\n",
        "    return tf.expand_dims(tf.convert_to_tensor(input_ids), 0), input_mask, tokens  "
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fm2VRL-ZX3dG",
        "outputId": "bd888c69-5f13-40d4-d9b5-338bbc1a205d"
      },
      "source": [
        "passage = \"My name is Bob.\"\r\n",
        "\r\n",
        "question = \"What is my name?\"\r\n",
        "\r\n",
        "input_ids, input_mask, tokens = prepare_bert_input(question, passage, tokenizer, 20)\r\n",
        "print(\"Test Case:\\n\")\r\n",
        "print(\"Passage: {}\".format(passage))\r\n",
        "print(\"Question: {}\".format(question))\r\n",
        "print()\r\n",
        "print(\"Tokens:\")\r\n",
        "print(tokens)\r\n",
        "print(\"\\nCorresponding input IDs:\")\r\n",
        "print(input_ids)\r\n",
        "print(\"\\nMask:\")\r\n",
        "print(input_mask)"
      ],
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Test Case:\n",
            "\n",
            "Passage: My name is Bob.\n",
            "Question: What is my name?\n",
            "\n",
            "Tokens:\n",
            "['[CLS]', 'what', 'is', 'my', 'name', '?', '[SEP]', 'my', 'name', 'is', 'bob', '.']\n",
            "\n",
            "Corresponding input IDs:\n",
            "tf.Tensor(\n",
            "[[ 101 2054 2003 2026 2171 1029  102 2026 2171 2003 3960 1012    0    0\n",
            "     0    0    0    0    0    0]], shape=(1, 20), dtype=int32)\n",
            "\n",
            "Mask:\n",
            "[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ovpD8XkfVpTU"
      },
      "source": [
        "#3.Download a BERT model and fine-tune it"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "m61IJ0VrVvPw"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H2cYcF0_Vvxv"
      },
      "source": [
        "#4.Evaluate the QA model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IoywxGkzVxO3"
      },
      "source": [
        "def get_span_from_scores(start_scores, end_scores, input_mask, verbose=False):\r\n",
        "    \"\"\"\r\n",
        "    Find start and end indices that maximize sum of start score\r\n",
        "    and end score, subject to the constraint that start is before end\r\n",
        "    and both are valid according to input_mask.\r\n",
        "\r\n",
        "    Args:\r\n",
        "        start_scores (list): contains scores for start positions, shape (1, n)\r\n",
        "        end_scores (list): constains scores for end positions, shape (1, n)\r\n",
        "        input_mask (list): 1 for valid positions and 0 otherwise\r\n",
        "    \"\"\"\r\n",
        "    n = len(start_scores)\r\n",
        "    max_start_i = -1\r\n",
        "    max_end_j = -1\r\n",
        "    max_start_score = -np.inf\r\n",
        "    max_end_score = -np.inf\r\n",
        "    max_sum = -np.inf\r\n",
        "    \r\n",
        "    # Find i and j that maximizes start_scores[i] + end_scores[j]\r\n",
        "    # so that i <= j and input_mask[i] == input_mask[j] == 1\r\n",
        "    \r\n",
        "    ### START CODE HERE (REPLACE INSTANCES OF 'None' with your code) ###\r\n",
        "    # set the range for i\r\n",
        "    for i in range(n): # complete this line\r\n",
        "        \r\n",
        "        # set the range for j\r\n",
        "        for j in range(i,n): #complete this line\r\n",
        "\r\n",
        "            # both input masks should be 1\r\n",
        "            if input_mask[i] == input_mask[j] == 1: # complete this line\r\n",
        "                \r\n",
        "                # check if the sum of the start and end scores is greater than the previous max sum\r\n",
        "                if start_scores[i] + end_scores[j] > max_sum: # complete this line\r\n",
        "\r\n",
        "                    # calculate the new max sum\r\n",
        "                    max_sum = start_scores[i] + end_scores[j]\r\n",
        "        \r\n",
        "                    # save the index of the max start score\r\n",
        "                    max_start_i = i\r\n",
        "                \r\n",
        "                    # save the index for the max end score\r\n",
        "                    max_end_j = j\r\n",
        "                    \r\n",
        "                    # save the value of the max start score\r\n",
        "                    max_start_val = start_scores[i]\r\n",
        "                    \r\n",
        "                    # save the value of the max end score\r\n",
        "                    max_end_val = end_scores[j]\r\n",
        "                                        \r\n",
        "    ### END CODE HERE ###\r\n",
        "    if verbose:\r\n",
        "        print(f\"max start is at index i={max_start_i} and score {max_start_val}\")\r\n",
        "        print(f\"max end is at index i={max_end_j} and score {max_end_val}\")\r\n",
        "        print(f\"max start + max end sum of scores is {max_sum}\")\r\n",
        "    return max_start_i, max_end_j"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-MWkZ00dX_9g"
      },
      "source": [
        "start_scores = tf.convert_to_tensor([-1, 2, 0.4, -0.3, 0, 8, 10, 12], dtype=float)\r\n",
        "end_scores = tf.convert_to_tensor([5, 1, 1, 3, 4, 10, 10, 10], dtype=float)\r\n",
        "input_mask = [1, 1, 1, 1, 1, 0, 0, 0]\r\n",
        "\r\n",
        "start, end = get_span_from_scores(start_scores, end_scores, input_mask, verbose=True)\r\n",
        "\r\n",
        "print(\"Expected: (1, 4) \\nReturned: ({}, {})\".format(start, end))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ffj6ze7HYA8n"
      },
      "source": [
        "# Test 2\r\n",
        "\r\n",
        "start_scores = tf.convert_to_tensor([0, 2, -1, 0.4, -0.3, 0, 8, 10, 12], dtype=float)\r\n",
        "end_scores = tf.convert_to_tensor([0, 5, 1, 1, 3, 4, 10, 10, 10], dtype=float)\r\n",
        "input_mask = [1, 1, 1, 1, 1, 0, 0, 0, 0 ]\r\n",
        "\r\n",
        "start, end = get_span_from_scores(start_scores, end_scores, input_mask, verbose=True)\r\n",
        "\r\n",
        "print(\"Expected: (1, 1) \\nReturned: ({}, {})\".format(start, end))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LAY_eV2iYCiz"
      },
      "source": [
        "# UNQ_C5 (UNIQUE CELL IDENTIFIER, DO NOT EDIT)\r\n",
        "def construct_answer(tokens):\r\n",
        "    \"\"\"\r\n",
        "    Combine tokens into a string, remove some hash symbols, and leading/trailing whitespace.\r\n",
        "    Args:\r\n",
        "        tokens: a list of tokens (strings)\r\n",
        "    \r\n",
        "    Returns:\r\n",
        "        out_string: the processed string.\r\n",
        "    \"\"\"\r\n",
        "    \r\n",
        "    ### START CODE HERE (REPLACE INSTANCES OF 'None' with your code) ###\r\n",
        "    \r\n",
        "    # join the tokens together with whitespace\r\n",
        "    out_string = \" \".join(tokens)\r\n",
        "    \r\n",
        "    # replace ' ##' with empty string\r\n",
        "    out_string = out_string.replace(' ##','')\r\n",
        "    \r\n",
        "    # remove leading and trailing whitespace\r\n",
        "    out_string = out_string.strip()\r\n",
        "\r\n",
        "    ### END CODE HERE ###\r\n",
        "    \r\n",
        "    # if there is an '@' symbol in the tokens, remove all whitespace\r\n",
        "    if '@' in tokens:\r\n",
        "        out_string = out_string.replace(' ', '')\r\n",
        "\r\n",
        "    return out_string"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8QHvFNh7YEKW"
      },
      "source": [
        "# Test\r\n",
        "\r\n",
        "tmp_tokens_1 = [' ## hello', 'how ', 'are ', 'you?      ']\r\n",
        "tmp_out_string_1 = construct_answer(tmp_tokens_1)\r\n",
        "\r\n",
        "print(f\"tmp_out_string_1: {tmp_out_string_1}, length {len(tmp_out_string_1)}\")\r\n",
        "\r\n",
        "\r\n",
        "tmp_tokens_2 = ['@',' ## hello', 'how ', 'are ', 'you?      ']\r\n",
        "tmp_out_string_2 = construct_answer(tmp_tokens_2)\r\n",
        "print(f\"tmp_out_string_2: {tmp_out_string_2}, length {len(tmp_out_string_2)}\")\r\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xqsEHqCuYGpd"
      },
      "source": [
        "def get_model_answer(model, question, passage, tokenizer, max_seq_length=384):\r\n",
        "    \"\"\"\r\n",
        "    Identify answer in passage for a given question using BERT. \r\n",
        "\r\n",
        "    Args:\r\n",
        "        model (Model): pretrained Bert model which we'll use to answer questions\r\n",
        "        question (string): question string\r\n",
        "        passage (string): passage string\r\n",
        "        tokenizer (Tokenizer): used for preprocessing of input\r\n",
        "        max_seq_length (int): length of input for model\r\n",
        "        \r\n",
        "    Returns:\r\n",
        "        answer (string): answer to input question according to model\r\n",
        "    \"\"\" \r\n",
        "    # prepare input: use the function prepare_bert_input\r\n",
        "    input_ids, input_mask, tokens = prepare_bert_input(question, passage, tokenizer, max_seq_length)\r\n",
        "    \r\n",
        "    # get scores for start of answer and end of answer\r\n",
        "    # use the model returned by TFAutoModelForQuestionAnswering.from_pretrained(\"./models\")\r\n",
        "    # pass in in the input ids that are returned by prepare_bert_input\r\n",
        "    start_scores, end_scores = model(input_ids)\r\n",
        "    \r\n",
        "    # start_scores and end_scores will be tensors of shape [1,max_seq_length]\r\n",
        "    # To pass these into get_span_from_scores function, \r\n",
        "    # take the value at index 0 to get a tensor of shape [max_seq_length]\r\n",
        "    start_scores = start_scores[0]\r\n",
        "    end_scores = end_scores[0]\r\n",
        "    \r\n",
        "    # using scores, get most likely answer\r\n",
        "    # use the get_span_from_scores function\r\n",
        "    span_start, span_end = get_span_from_scores(start_scores, end_scores, input_mask)\r\n",
        "    \r\n",
        "    # Using array indexing to get the tokens from the span start to span end (including the span_end)\r\n",
        "    answer_tokens = tokens[span_start:span_end+1]\r\n",
        "    \r\n",
        "    # Combine the tokens into a single string and perform post-processing\r\n",
        "    # use construct_answer\r\n",
        "    answer = construct_answer(answer_tokens)\r\n",
        "    \r\n",
        "    return answer"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FZavI0NGYISI"
      },
      "source": [
        "passage = \"Computational complexity theory is a branch of the theory \\\r\n",
        "           of computation in theoretical computer science that focuses \\\r\n",
        "           on classifying computational problems according to their inherent \\\r\n",
        "           difficulty, and relating those classes to each other. A computational \\\r\n",
        "           problem is understood to be a task that is in principle amenable to \\\r\n",
        "           being solved by a computer, which is equivalent to stating that the \\\r\n",
        "           problem may be solved by mechanical application of mathematical steps, \\\r\n",
        "           such as an algorithm.\"\r\n",
        "\r\n",
        "question = \"What branch of theoretical computer science deals with broadly \\\r\n",
        "            classifying computational problems by difficulty and class of relationship?\"\r\n",
        "\r\n",
        "print(\"Output: {}\".format(get_model_answer(model, question, passage, tokenizer)))\r\n",
        "print(\"Expected: Computational complexity theory\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nGvk9SiYYJaE"
      },
      "source": [
        "passage = \"The word pharmacy is derived from its root word pharma which was a term used since \\\r\n",
        "           the 15th–17th centuries. However, the original Greek roots from pharmakos imply sorcery \\\r\n",
        "           or even poison. In addition to pharma responsibilities, the pharma offered general medical \\\r\n",
        "           advice and a range of services that are now performed solely by other specialist practitioners, \\\r\n",
        "           such as surgery and midwifery. The pharma (as it was referred to) often operated through a \\\r\n",
        "           retail shop which, in addition to ingredients for medicines, sold tobacco and patent medicines. \\\r\n",
        "           Often the place that did this was called an apothecary and several languages have this as the \\\r\n",
        "           dominant term, though their practices are more akin to a modern pharmacy, in English the term \\\r\n",
        "           apothecary would today be seen as outdated or only approproriate if herbal remedies were on offer \\\r\n",
        "           to a large extent. The pharmas also used many other herbs not listed. The Greek word Pharmakeia \\\r\n",
        "           (Greek: φαρμακεία) derives from pharmakon (φάρμακον), meaning 'drug', 'medicine' (or 'poison').\"\r\n",
        "\r\n",
        "question = \"What word is the word pharmacy taken from?\"\r\n",
        "\r\n",
        "print(\"Output: {}\".format(get_model_answer(model, question, passage, tokenizer)))\r\n",
        "print(\"Expected: pharma\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "k-l6i30iYKy4"
      },
      "source": [
        "passage = \"Abnormal echocardiogram findings and followup. Shortness of breath, congestive heart failure, \\\r\n",
        "           and valvular insufficiency. The patient complains of shortness of breath, which is worsening. \\\r\n",
        "           The patient underwent an echocardiogram, which shows severe mitral regurgitation and also large \\\r\n",
        "           pleural effusion. The patient is an 86-year-old female admitted for evaluation of abdominal pain \\\r\n",
        "           and bloody stools. The patient has colitis and also diverticulitis, undergoing treatment. \\\r\n",
        "           During the hospitalization, the patient complains of shortness of breath, which is worsening. \\\r\n",
        "           The patient underwent an echocardiogram, which shows severe mitral regurgitation and also large \\\r\n",
        "           pleural effusion. This consultation is for further evaluation in this regard. As per the patient, \\\r\n",
        "           she is an 86-year-old female, has limited activity level. She has been having shortness of breath \\\r\n",
        "           for many years. She also was told that she has a heart murmur, which was not followed through \\\r\n",
        "           on a regular basis.\"\r\n",
        "\r\n",
        "q1 = \"How old is the patient?\"\r\n",
        "q2 = \"Does the patient have any complaints?\"\r\n",
        "q3 = \"What is the reason for this consultation?\"\r\n",
        "q4 = \"What does her echocardiogram show?\"\r\n",
        "q5 = \"What other symptoms does the patient have?\"\r\n",
        "q6 = \"What is the gender of this patient?\"\r\n",
        "\r\n",
        "questions = [q6]\r\n",
        "\r\n",
        "for i, q in enumerate(questions):\r\n",
        "    print(\"Question {}: {}\".format(i+1, q))\r\n",
        "    print()\r\n",
        "    print(\"Answer: {}\".format(get_model_answer(model, q, passage, tokenizer)))\r\n",
        "    print()\r\n",
        "    print()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CkHR8UBXVxyv"
      },
      "source": [
        "#5.Deploy the model on Django"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wZO4HHHKV1UO"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}