{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "QA BERT model.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyNscM8PYyxYkh8JodRJ6nqH",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/supertime1/BERT/blob/main/QA_BERT_model.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xz4W_-pMVJsX"
      },
      "source": [
        "#1.Import Dependencies"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "orAMKxKkU_Vh"
      },
      "source": [
        "import os\r\n",
        "import tensorflow as tf\r\n",
        "import tensorflow_hub as hub\r\n",
        "import tensorflow_datasets as tfds\r\n",
        "import tensorflow_text as text  # A dependency of the preprocessing model\r\n",
        "import tensorflow_addons as tfa\r\n",
        "from official.nlp import optimization\r\n",
        "import numpy as np\r\n",
        "import json\r\n",
        "\r\n",
        "tf.get_logger().setLevel('ERROR')"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8xr0VeYgYyKp"
      },
      "source": [
        "import sys\r\n",
        "sys.path.append(r'C:\\Users\\57lzhang.US04WW4008\\Documents\\GitHub\\BERT')"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Zlt53u2UVlb-"
      },
      "source": [
        "#2.Preprocess the input data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_2Ko_XCSfGRv"
      },
      "source": [
        "import json\r\n",
        "from pathlib import Path\r\n",
        "\r\n",
        "def read_squad(path):\r\n",
        "    path = Path(path)\r\n",
        "    with open(path, 'rb') as f:\r\n",
        "        squad_dict = json.load(f)\r\n",
        "\r\n",
        "    contexts = []\r\n",
        "    questions = []\r\n",
        "    answers = []\r\n",
        "    for group in squad_dict['data']:\r\n",
        "        for passage in group['paragraphs']:\r\n",
        "            context = passage['context']\r\n",
        "            for qa in passage['qas']:\r\n",
        "                question = qa['question']\r\n",
        "                for answer in qa['answers']:\r\n",
        "                    contexts.append(context)\r\n",
        "                    questions.append(question)\r\n",
        "                    answers.append(answer)\r\n",
        "\r\n",
        "    return contexts, questions, answers\r\n",
        "\r\n",
        "train_contexts, train_questions, train_answers = read_squad(r'C:\\Users\\57lzhang.US04WW4008\\Documents\\GitHub\\BERT\\dataset\\train-v2.0.json')\r\n",
        "val_contexts, val_questions, val_answers = read_squad(r'C:\\Users\\57lzhang.US04WW4008\\Documents\\GitHub\\BERT\\dataset\\dev-v2.0.json')"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7YNrDUNnhKTL"
      },
      "source": [
        "def add_end_idx(answers, contexts):\r\n",
        "    for answer, context in zip(answers, contexts):\r\n",
        "        gold_text = answer['text']\r\n",
        "        start_idx = answer['answer_start']\r\n",
        "        end_idx = start_idx + len(gold_text)\r\n",
        "\r\n",
        "        # sometimes squad answers are off by a character or two – fix this\r\n",
        "        if context[start_idx:end_idx] == gold_text:\r\n",
        "            answer['answer_end'] = end_idx\r\n",
        "        elif context[start_idx-1:end_idx-1] == gold_text:\r\n",
        "            answer['answer_start'] = start_idx - 1\r\n",
        "            answer['answer_end'] = end_idx - 1     # When the gold label is off by one character\r\n",
        "        elif context[start_idx-2:end_idx-2] == gold_text:\r\n",
        "            answer['answer_start'] = start_idx - 2\r\n",
        "            answer['answer_end'] = end_idx - 2     # When the gold label is off by two characters\r\n",
        "\r\n",
        "add_end_idx(train_answers, train_contexts)\r\n",
        "add_end_idx(val_answers, val_contexts)"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5P3ccTLKtU62",
        "outputId": "a12271f7-e12a-4358-fa24-e96db3d9f9ef"
      },
      "source": [
        "train_answers[0]"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'text': 'in the late 1990s', 'answer_start': 269, 'answer_end': 286}"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rc0LtV2ahOsd"
      },
      "source": [
        "from transformers import DistilBertTokenizerFast\r\n",
        "tokenizer = DistilBertTokenizerFast.from_pretrained('distilbert-base-uncased')\r\n",
        "\r\n",
        "train_encodings = tokenizer(train_contexts, train_questions, truncation=True, padding=True)\r\n",
        "val_encodings = tokenizer(val_contexts, val_questions, truncation=True, padding=True)"
      ],
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MC5IQyTekTYF",
        "outputId": "72b8cb62-f919-41b0-a9b8-963ced19032b"
      },
      "source": [
        "train_encodings.keys()"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "dict_keys(['input_ids', 'attention_mask'])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DX1rS60ThqV-"
      },
      "source": [
        "def add_token_positions(encodings, answers):\r\n",
        "    start_positions = []\r\n",
        "    end_positions = []\r\n",
        "    for i in range(len(answers)):\r\n",
        "        start_positions.append(encodings.char_to_token(i, answers[i]['answer_start']))\r\n",
        "        end_positions.append(encodings.char_to_token(i, answers[i]['answer_end'] - 1))\r\n",
        "        # if None, the answer passage has been truncated\r\n",
        "        if start_positions[-1] is None:\r\n",
        "            start_positions[-1] = tokenizer.model_max_length\r\n",
        "        if end_positions[-1] is None:\r\n",
        "            end_positions[-1] = tokenizer.model_max_length\r\n",
        "    encodings.update({'start_positions': start_positions, 'end_positions': end_positions})\r\n",
        "\r\n",
        "add_token_positions(train_encodings, train_answers)\r\n",
        "add_token_positions(val_encodings, val_answers)"
      ],
      "execution_count": 46,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PxOJA4dUxTwf"
      },
      "source": [
        "import tensorflow as tf\r\n",
        "\r\n",
        "train_dataset = tf.data.Dataset.from_tensor_slices((\r\n",
        "    {key: train_encodings[key] for key in ['input_ids', 'attention_mask']},\r\n",
        "    {key: train_encodings[key] for key in ['start_positions', 'end_positions']}\r\n",
        "))\r\n",
        "val_dataset = tf.data.Dataset.from_tensor_slices((\r\n",
        "    {key: val_encodings[key] for key in ['input_ids', 'attention_mask']},\r\n",
        "    {key: val_encodings[key] for key in ['start_positions', 'end_positions']}\r\n",
        "))"
      ],
      "execution_count": 47,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ovpD8XkfVpTU"
      },
      "source": [
        "#3.Download a BERT model and fine-tune it"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "m61IJ0VrVvPw"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H2cYcF0_Vvxv"
      },
      "source": [
        "#4.Evaluate the QA model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IoywxGkzVxO3"
      },
      "source": [
        "def get_span_from_scores(start_scores, end_scores, input_mask, verbose=False):\r\n",
        "    \"\"\"\r\n",
        "    Find start and end indices that maximize sum of start score\r\n",
        "    and end score, subject to the constraint that start is before end\r\n",
        "    and both are valid according to input_mask.\r\n",
        "\r\n",
        "    Args:\r\n",
        "        start_scores (list): contains scores for start positions, shape (1, n)\r\n",
        "        end_scores (list): constains scores for end positions, shape (1, n)\r\n",
        "        input_mask (list): 1 for valid positions and 0 otherwise\r\n",
        "    \"\"\"\r\n",
        "    n = len(start_scores)\r\n",
        "    max_start_i = -1\r\n",
        "    max_end_j = -1\r\n",
        "    max_start_score = -np.inf\r\n",
        "    max_end_score = -np.inf\r\n",
        "    max_sum = -np.inf\r\n",
        "    \r\n",
        "    # Find i and j that maximizes start_scores[i] + end_scores[j]\r\n",
        "    # so that i <= j and input_mask[i] == input_mask[j] == 1\r\n",
        "    \r\n",
        "    ### START CODE HERE (REPLACE INSTANCES OF 'None' with your code) ###\r\n",
        "    # set the range for i\r\n",
        "    for i in range(n): # complete this line\r\n",
        "        \r\n",
        "        # set the range for j\r\n",
        "        for j in range(i,n): #complete this line\r\n",
        "\r\n",
        "            # both input masks should be 1\r\n",
        "            if input_mask[i] == input_mask[j] == 1: # complete this line\r\n",
        "                \r\n",
        "                # check if the sum of the start and end scores is greater than the previous max sum\r\n",
        "                if start_scores[i] + end_scores[j] > max_sum: # complete this line\r\n",
        "\r\n",
        "                    # calculate the new max sum\r\n",
        "                    max_sum = start_scores[i] + end_scores[j]\r\n",
        "        \r\n",
        "                    # save the index of the max start score\r\n",
        "                    max_start_i = i\r\n",
        "                \r\n",
        "                    # save the index for the max end score\r\n",
        "                    max_end_j = j\r\n",
        "                    \r\n",
        "                    # save the value of the max start score\r\n",
        "                    max_start_val = start_scores[i]\r\n",
        "                    \r\n",
        "                    # save the value of the max end score\r\n",
        "                    max_end_val = end_scores[j]\r\n",
        "                                        \r\n",
        "    ### END CODE HERE ###\r\n",
        "    if verbose:\r\n",
        "        print(f\"max start is at index i={max_start_i} and score {max_start_val}\")\r\n",
        "        print(f\"max end is at index i={max_end_j} and score {max_end_val}\")\r\n",
        "        print(f\"max start + max end sum of scores is {max_sum}\")\r\n",
        "    return max_start_i, max_end_j"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-MWkZ00dX_9g"
      },
      "source": [
        "start_scores = tf.convert_to_tensor([-1, 2, 0.4, -0.3, 0, 8, 10, 12], dtype=float)\r\n",
        "end_scores = tf.convert_to_tensor([5, 1, 1, 3, 4, 10, 10, 10], dtype=float)\r\n",
        "input_mask = [1, 1, 1, 1, 1, 0, 0, 0]\r\n",
        "\r\n",
        "start, end = get_span_from_scores(start_scores, end_scores, input_mask, verbose=True)\r\n",
        "\r\n",
        "print(\"Expected: (1, 4) \\nReturned: ({}, {})\".format(start, end))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ffj6ze7HYA8n"
      },
      "source": [
        "# Test 2\r\n",
        "\r\n",
        "start_scores = tf.convert_to_tensor([0, 2, -1, 0.4, -0.3, 0, 8, 10, 12], dtype=float)\r\n",
        "end_scores = tf.convert_to_tensor([0, 5, 1, 1, 3, 4, 10, 10, 10], dtype=float)\r\n",
        "input_mask = [1, 1, 1, 1, 1, 0, 0, 0, 0 ]\r\n",
        "\r\n",
        "start, end = get_span_from_scores(start_scores, end_scores, input_mask, verbose=True)\r\n",
        "\r\n",
        "print(\"Expected: (1, 1) \\nReturned: ({}, {})\".format(start, end))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LAY_eV2iYCiz"
      },
      "source": [
        "# UNQ_C5 (UNIQUE CELL IDENTIFIER, DO NOT EDIT)\r\n",
        "def construct_answer(tokens):\r\n",
        "    \"\"\"\r\n",
        "    Combine tokens into a string, remove some hash symbols, and leading/trailing whitespace.\r\n",
        "    Args:\r\n",
        "        tokens: a list of tokens (strings)\r\n",
        "    \r\n",
        "    Returns:\r\n",
        "        out_string: the processed string.\r\n",
        "    \"\"\"\r\n",
        "    \r\n",
        "    ### START CODE HERE (REPLACE INSTANCES OF 'None' with your code) ###\r\n",
        "    \r\n",
        "    # join the tokens together with whitespace\r\n",
        "    out_string = \" \".join(tokens)\r\n",
        "    \r\n",
        "    # replace ' ##' with empty string\r\n",
        "    out_string = out_string.replace(' ##','')\r\n",
        "    \r\n",
        "    # remove leading and trailing whitespace\r\n",
        "    out_string = out_string.strip()\r\n",
        "\r\n",
        "    ### END CODE HERE ###\r\n",
        "    \r\n",
        "    # if there is an '@' symbol in the tokens, remove all whitespace\r\n",
        "    if '@' in tokens:\r\n",
        "        out_string = out_string.replace(' ', '')\r\n",
        "\r\n",
        "    return out_string"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8QHvFNh7YEKW"
      },
      "source": [
        "# Test\r\n",
        "\r\n",
        "tmp_tokens_1 = [' ## hello', 'how ', 'are ', 'you?      ']\r\n",
        "tmp_out_string_1 = construct_answer(tmp_tokens_1)\r\n",
        "\r\n",
        "print(f\"tmp_out_string_1: {tmp_out_string_1}, length {len(tmp_out_string_1)}\")\r\n",
        "\r\n",
        "\r\n",
        "tmp_tokens_2 = ['@',' ## hello', 'how ', 'are ', 'you?      ']\r\n",
        "tmp_out_string_2 = construct_answer(tmp_tokens_2)\r\n",
        "print(f\"tmp_out_string_2: {tmp_out_string_2}, length {len(tmp_out_string_2)}\")\r\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xqsEHqCuYGpd"
      },
      "source": [
        "def get_model_answer(model, question, passage, tokenizer, max_seq_length=384):\r\n",
        "    \"\"\"\r\n",
        "    Identify answer in passage for a given question using BERT. \r\n",
        "\r\n",
        "    Args:\r\n",
        "        model (Model): pretrained Bert model which we'll use to answer questions\r\n",
        "        question (string): question string\r\n",
        "        passage (string): passage string\r\n",
        "        tokenizer (Tokenizer): used for preprocessing of input\r\n",
        "        max_seq_length (int): length of input for model\r\n",
        "        \r\n",
        "    Returns:\r\n",
        "        answer (string): answer to input question according to model\r\n",
        "    \"\"\" \r\n",
        "    # prepare input: use the function prepare_bert_input\r\n",
        "    input_ids, input_mask, tokens = prepare_bert_input(question, passage, tokenizer, max_seq_length)\r\n",
        "    \r\n",
        "    # get scores for start of answer and end of answer\r\n",
        "    # use the model returned by TFAutoModelForQuestionAnswering.from_pretrained(\"./models\")\r\n",
        "    # pass in in the input ids that are returned by prepare_bert_input\r\n",
        "    start_scores, end_scores = model(input_ids)\r\n",
        "    \r\n",
        "    # start_scores and end_scores will be tensors of shape [1,max_seq_length]\r\n",
        "    # To pass these into get_span_from_scores function, \r\n",
        "    # take the value at index 0 to get a tensor of shape [max_seq_length]\r\n",
        "    start_scores = start_scores[0]\r\n",
        "    end_scores = end_scores[0]\r\n",
        "    \r\n",
        "    # using scores, get most likely answer\r\n",
        "    # use the get_span_from_scores function\r\n",
        "    span_start, span_end = get_span_from_scores(start_scores, end_scores, input_mask)\r\n",
        "    \r\n",
        "    # Using array indexing to get the tokens from the span start to span end (including the span_end)\r\n",
        "    answer_tokens = tokens[span_start:span_end+1]\r\n",
        "    \r\n",
        "    # Combine the tokens into a single string and perform post-processing\r\n",
        "    # use construct_answer\r\n",
        "    answer = construct_answer(answer_tokens)\r\n",
        "    \r\n",
        "    return answer"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FZavI0NGYISI"
      },
      "source": [
        "passage = \"Computational complexity theory is a branch of the theory \\\r\n",
        "           of computation in theoretical computer science that focuses \\\r\n",
        "           on classifying computational problems according to their inherent \\\r\n",
        "           difficulty, and relating those classes to each other. A computational \\\r\n",
        "           problem is understood to be a task that is in principle amenable to \\\r\n",
        "           being solved by a computer, which is equivalent to stating that the \\\r\n",
        "           problem may be solved by mechanical application of mathematical steps, \\\r\n",
        "           such as an algorithm.\"\r\n",
        "\r\n",
        "question = \"What branch of theoretical computer science deals with broadly \\\r\n",
        "            classifying computational problems by difficulty and class of relationship?\"\r\n",
        "\r\n",
        "print(\"Output: {}\".format(get_model_answer(model, question, passage, tokenizer)))\r\n",
        "print(\"Expected: Computational complexity theory\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nGvk9SiYYJaE"
      },
      "source": [
        "passage = \"The word pharmacy is derived from its root word pharma which was a term used since \\\r\n",
        "           the 15th–17th centuries. However, the original Greek roots from pharmakos imply sorcery \\\r\n",
        "           or even poison. In addition to pharma responsibilities, the pharma offered general medical \\\r\n",
        "           advice and a range of services that are now performed solely by other specialist practitioners, \\\r\n",
        "           such as surgery and midwifery. The pharma (as it was referred to) often operated through a \\\r\n",
        "           retail shop which, in addition to ingredients for medicines, sold tobacco and patent medicines. \\\r\n",
        "           Often the place that did this was called an apothecary and several languages have this as the \\\r\n",
        "           dominant term, though their practices are more akin to a modern pharmacy, in English the term \\\r\n",
        "           apothecary would today be seen as outdated or only approproriate if herbal remedies were on offer \\\r\n",
        "           to a large extent. The pharmas also used many other herbs not listed. The Greek word Pharmakeia \\\r\n",
        "           (Greek: φαρμακεία) derives from pharmakon (φάρμακον), meaning 'drug', 'medicine' (or 'poison').\"\r\n",
        "\r\n",
        "question = \"What word is the word pharmacy taken from?\"\r\n",
        "\r\n",
        "print(\"Output: {}\".format(get_model_answer(model, question, passage, tokenizer)))\r\n",
        "print(\"Expected: pharma\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "k-l6i30iYKy4"
      },
      "source": [
        "passage = \"Abnormal echocardiogram findings and followup. Shortness of breath, congestive heart failure, \\\r\n",
        "           and valvular insufficiency. The patient complains of shortness of breath, which is worsening. \\\r\n",
        "           The patient underwent an echocardiogram, which shows severe mitral regurgitation and also large \\\r\n",
        "           pleural effusion. The patient is an 86-year-old female admitted for evaluation of abdominal pain \\\r\n",
        "           and bloody stools. The patient has colitis and also diverticulitis, undergoing treatment. \\\r\n",
        "           During the hospitalization, the patient complains of shortness of breath, which is worsening. \\\r\n",
        "           The patient underwent an echocardiogram, which shows severe mitral regurgitation and also large \\\r\n",
        "           pleural effusion. This consultation is for further evaluation in this regard. As per the patient, \\\r\n",
        "           she is an 86-year-old female, has limited activity level. She has been having shortness of breath \\\r\n",
        "           for many years. She also was told that she has a heart murmur, which was not followed through \\\r\n",
        "           on a regular basis.\"\r\n",
        "\r\n",
        "q1 = \"How old is the patient?\"\r\n",
        "q2 = \"Does the patient have any complaints?\"\r\n",
        "q3 = \"What is the reason for this consultation?\"\r\n",
        "q4 = \"What does her echocardiogram show?\"\r\n",
        "q5 = \"What other symptoms does the patient have?\"\r\n",
        "q6 = \"What is the gender of this patient?\"\r\n",
        "\r\n",
        "questions = [q6]\r\n",
        "\r\n",
        "for i, q in enumerate(questions):\r\n",
        "    print(\"Question {}: {}\".format(i+1, q))\r\n",
        "    print()\r\n",
        "    print(\"Answer: {}\".format(get_model_answer(model, q, passage, tokenizer)))\r\n",
        "    print()\r\n",
        "    print()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CkHR8UBXVxyv"
      },
      "source": [
        "#5.Deploy the model on Django"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wZO4HHHKV1UO"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}